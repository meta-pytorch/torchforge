# Multi-Node SFT Configuration for Qwen3-32B
# >>> python -m apps.sft.main --config apps/sft/qwen3_32b_multinode.yaml

comm:
  trace_buf_size: 0

model_name: "Qwen/Qwen3-32B"

provisioner:
  launcher: slurm
  cpu:             # CPUs per node - if emtpy, will be inferred from Slurm
  memory_mb:   # Memory in MB per node - if emtpy, will be inferred from Slurm
  gpus_per_node:     # Number of GPUs per node - if emtpy, will be inferred from Slurm

# Actor configuration for multi-node training
actors:
  trainer:
    procs: 4              # Number of GPU processes per node
    hosts: 64             # Number of nodes to use
    with_gpus: true
    mesh_name: trainer

model:
  name: qwen3
  flavor: 32B
  hf_assets_path: hf://${model}

optimizer:
  name: AdamW
  lr: 1e-5
  eps: 1e-8

lr_scheduler:
  warmup_steps: 200

training:
  local_batch_size: 1
  seq_len: 2048
  max_norm: 1.0
  steps: 1000000
  compile: false
  dataset: "c4"


data:
  # This is needed to be adjusted based on the dataset size and world size - sample size >= world size * num_shards_per_rank
  num_shards_per_rank: 64   # Default: 64
  num_dataloader_workers: 0  # 0 = no worker processes


parallelism:
  data_parallel_replicate_degree: 1
  data_parallel_shard_degree: -1
  tensor_parallel_degree: 1
  pipeline_parallel_degree: 1
  context_parallel_degree: 1
  expert_parallel_degree: 1
  disable_loss_parallel: false

checkpoint:
  enable: true
  folder: ./checkpoints
  # To fine-tune from pre-trained HF model (base model), uncomment these:
  initial_load_path: hf://${model}
  initial_load_in_hf: true
  last_save_in_hf: true
  interval: 500
  async_mode: "disabled"  # Save checkpoints in background without blocking training

activation_checkpoint:
  mode: full

# Metric logging configuration
metric_logging:
  wandb:
    project: sft-training
    group: sft_exp_${oc.env:USER}
    logging_mode: global_reduce    #global_reduce, per_rank_reduce, per_rank_no_reduce
  # console:
  #   reduce_across_ranks: True

# Optional: Profiling configuration
# profiling:
#   enable_profiling: false

# Optional: Metrics configuration
# metrics:
#   log_freq: 10
#   enable_tensorboard: true
#   save_tb_folder: "tb"
