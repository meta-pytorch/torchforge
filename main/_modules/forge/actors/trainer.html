
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>forge.actors.trainer &#8212; torchforge 0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=a8c4d769" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=2709fde1"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/forge/actors/trainer';</script>
    <script src="../../../_static/custom.js?v=b1073d4a"></script>
    <link rel="canonical" href="https://meta-pytorch.org/forge/main/_modules/forge/actors/trainer.html" />
    <link rel="icon" href="../../../_static/logo-icon.svg"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
</style>


<style>
  @media (min-width: 1100px) {
    .site-footer {
      height: 300px !important;
    }
  }
</style>

<link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../../../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NPLPKN5G" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-NPLPKN5G');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', '');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<script>
  // Define repository configuration for tutorial buttons using existing html_context variables
  // Only injected when tutorial buttons are shown AND github variables are defined
  // If either condition is false, JavaScript will fallback to default PyTorch tutorial links
  window.repoConfig = {
    github_repo: "meta-pytorch/forge",
    github_branch: "main",
    colab_repo: "meta-pytorch/forge",
    colab_branch: "gh-pages"
  };
</script>

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/meta-pytorch/forge" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../getting_started.html">
    Get Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../concepts.html">
    Concepts
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/forge" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchforge/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../getting_started.html">
    Get Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../concepts.html">
    Concepts
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/forge" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchforge/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">forge.actors.trainer</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../../index.html">
        <meta itemprop="name" content="Module code">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="forge.actors.trainer">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">_modules/forge/actors/trainer</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../../../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../../../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../../../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for forge.actors.trainer</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD-style license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Mapping</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span><span class="p">,</span> <span class="n">fields</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed.checkpoint</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dcp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchstore</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ts</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">monarch.actor</span><span class="w"> </span><span class="kn">import</span> <span class="n">endpoint</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.checkpoint._nested_dict</span><span class="w"> </span><span class="kn">import</span> <span class="n">flatten_state_dict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchtitan.config.job_config</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">ActivationCheckpoint</span><span class="p">,</span>
    <span class="n">Checkpoint</span><span class="p">,</span>
    <span class="n">Comm</span><span class="p">,</span>
    <span class="n">Compile</span><span class="p">,</span>
    <span class="n">Job</span><span class="p">,</span>
    <span class="n">LRScheduler</span><span class="p">,</span>
    <span class="n">MemoryEstimation</span><span class="p">,</span>
    <span class="n">Model</span><span class="p">,</span>
    <span class="n">Optimizer</span><span class="p">,</span>
    <span class="n">Parallelism</span><span class="p">,</span>
    <span class="n">Quantize</span><span class="p">,</span>
    <span class="n">Training</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchtitan.experiments.forge.engine</span><span class="w"> </span><span class="kn">import</span> <span class="n">ForgeEngine</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchtitan.experiments.forge.job_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">ForgeJobConfig</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">forge.actors._torchstore_utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">DcpHandle</span><span class="p">,</span>
    <span class="n">get_dcp_whole_state_dict_key</span><span class="p">,</span>
    <span class="n">get_param_key</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">forge.controller</span><span class="w"> </span><span class="kn">import</span> <span class="n">ForgeActor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">forge.data.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">batch_to_device</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">forge.env</span><span class="w"> </span><span class="kn">import</span> <span class="n">TORCHSTORE_USE_RDMA</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">forge.observability.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">record_metric</span><span class="p">,</span> <span class="n">Reduce</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">forge.observability.perf_tracker</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tracer</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">cleanup_old_weight_versions</span><span class="p">(</span>
    <span class="n">state_dict_key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">delim</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">current_policy_version</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Delete old weight versions, keeping only current and N-1 versions.</span>

<span class="sd">    TODO - issues/194: provide a more robust way to handle eviction.</span>

<span class="sd">    Args:</span>
<span class="sd">        state_dict_key: The base key for state dict storage</span>
<span class="sd">        delim: The delimiter used between key and version</span>
<span class="sd">        current_policy_version: The current policy version to keep</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">current_policy_version</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span>  <span class="c1"># No cleanup needed for versions 0 or 1</span>

    <span class="n">prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">state_dict_key</span><span class="si">}{</span><span class="n">delim</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">current_weights</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">current_policy_version</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">previous_weights</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">current_policy_version</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="c1"># Find all weight directories that match our pattern</span>
    <span class="n">parent_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">&quot;.&quot;</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">parent_dir</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">parent_dir</span><span class="p">):</span>
            <span class="n">item_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">parent_dir</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">item</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">prefix</span><span class="p">))</span>
                <span class="ow">and</span> <span class="n">item</span> <span class="o">!=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">current_weights</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">item</span> <span class="o">!=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">previous_weights</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">item_path</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">item_path</span><span class="p">,</span> <span class="n">ignore_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Removed old weights at </span><span class="si">{</span><span class="n">item_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">OSError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error deleting </span><span class="si">{</span><span class="n">item_path</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="RLTrainer">
<a class="viewcode-back" href="../../../api_trainer.html#forge.actors.trainer.RLTrainer">[docs]</a>
<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">RLTrainer</span><span class="p">(</span><span class="n">ForgeActor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A reinforcement learning trainer actor for policy optimization training.</span>

<span class="sd">    Built on top of TorchTitan&#39;s training engine, this actor provides a complete training</span>
<span class="sd">    loop for reinforcement learning. It performs forward and backward passes with gradient</span>
<span class="sd">    computation, optimization steps, and checkpoint management. Unlike the ReferenceModel</span>
<span class="sd">    actor which only runs forward passes, RLTrainer actively updates the policy model</span>
<span class="sd">    parameters through gradient descent.</span>

<span class="sd">    The trainer supports the same distributed training strategies that TorchTitan does,</span>
<span class="sd">    including but not limited to, tensor parallelism, data parallelism, and FSDP</span>
<span class="sd">    (Fully Sharded Data Parallel). It is typically used in conjunction with ReferenceModel</span>
<span class="sd">    for policy optimization algorithms like GRPO (Group Relative Policy Optimization),</span>
<span class="sd">    where it optimizes the policy against a loss that includes KL divergence penalties</span>
<span class="sd">    from the reference model.</span>

<span class="sd">    The trainer handles:</span>
<span class="sd">    - Forward and backward propagation with automatic mixed precision (AMP)</span>
<span class="sd">    - Optimizer steps with learning rate scheduling</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">job</span><span class="p">:</span> <span class="n">Job</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Job</span><span class="p">)</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Model</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Model</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Optimizer</span><span class="p">)</span>
    <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">LRScheduler</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">LRScheduler</span><span class="p">)</span>
    <span class="n">training</span><span class="p">:</span> <span class="n">Training</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Training</span><span class="p">)</span>
    <span class="n">parallelism</span><span class="p">:</span> <span class="n">Parallelism</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Parallelism</span><span class="p">)</span>
    <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Checkpoint</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Checkpoint</span><span class="p">)</span>
    <span class="n">activation_checkpoint</span><span class="p">:</span> <span class="n">ActivationCheckpoint</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="n">ActivationCheckpoint</span>
    <span class="p">)</span>
    <span class="nb">compile</span><span class="p">:</span> <span class="n">Compile</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Compile</span><span class="p">)</span>
    <span class="n">quantize</span><span class="p">:</span> <span class="n">Quantize</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Quantize</span><span class="p">)</span>
    <span class="n">comm</span><span class="p">:</span> <span class="n">Comm</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">Comm</span><span class="p">)</span>
    <span class="n">memory_estimation</span><span class="p">:</span> <span class="n">MemoryEstimation</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">MemoryEstimation</span><span class="p">)</span>
    <span class="c1"># Non JobConfig-related fields</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">logits</span><span class="p">,</span> <span class="o">**</span><span class="n">targets</span><span class="p">:</span> <span class="n">logits</span>
    <span class="n">state_dict_key</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;model_state_dict&quot;</span>
    <span class="n">use_dcp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">TORCHSTORE_USE_RDMA</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="p">)</span>  <span class="c1"># torchstore currently only accepts 0 or 1</span>
    <span class="n">dcp_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;forge_dcp_tmp&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes config types and env variables.</span>

<span class="sd">        torchrun normally hands env variables, but we need to do it ourselves</span>
<span class="sd">        in monarch for now.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_dcp</span><span class="p">:</span>
            <span class="c1"># DCP specific optimization</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">set_crc32_options</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Instantiate dict fields</span>
        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">fields</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">attr</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">):</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="o">**</span><span class="n">attr</span><span class="p">))</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">type</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> should be a </span><span class="si">{</span><span class="n">f</span><span class="o">.</span><span class="n">type</span><span class="si">}</span><span class="s2"> type or a dict like object&quot;</span>
                <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># fragile contract.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_training_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYTORCH_CUDA_ALLOC_CONF&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;expandable_segments:True&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Compiling loss&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># TODO: update ForgeEngine to not use ForgeJobConfig</span>
        <span class="n">engine_config</span> <span class="o">=</span> <span class="p">{</span><span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">fields</span><span class="p">(</span><span class="bp">self</span><span class="p">)}</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">{</span>
            <span class="s2">&quot;loss&quot;</span><span class="p">,</span>
            <span class="s2">&quot;state_dict_key&quot;</span><span class="p">,</span>
            <span class="s2">&quot;use_dcp&quot;</span><span class="p">,</span>
            <span class="s2">&quot;dcp_path&quot;</span><span class="p">,</span>
        <span class="p">}:</span>
            <span class="n">engine_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>  <span class="c1"># Not part of job config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span> <span class="o">=</span> <span class="n">ForgeEngine</span><span class="p">(</span><span class="n">ForgeJobConfig</span><span class="p">(</span><span class="o">**</span><span class="n">engine_config</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<div class="viewcode-block" id="RLTrainer.forward_backward">
<a class="viewcode-back" href="../../../api_trainer.html#forge.actors.trainer.RLTrainer.forward_backward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward_backward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">targets</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">model_parts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">model_parts</span>
        <span class="n">parallel_dims</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">parallel_dims</span>

        <span class="c1"># apply context parallelism if cp is enabled</span>
        <span class="c1"># ensure CP handles the separate freqs_cis buffer for each pp stage</span>
        <span class="c1"># if getattr(self.engine.model_args, &quot;use_flex_attn&quot;, False):</span>
        <span class="c1">#     cp_mesh = (</span>
        <span class="c1">#         parallel_dims.world_mesh[&quot;cp&quot;] if parallel_dims.cp_enabled else None</span>
        <span class="c1">#     )</span>
        <span class="c1">#     init_attention_mask(</span>
        <span class="c1">#         inputs, self.engine.tokenizer.base_tokenizer.eos_id, cp_mesh</span>
        <span class="c1">#     )</span>

        <span class="c1"># optional_context_parallel_ctx = (</span>
        <span class="c1">#     dist_utils.create_context_parallel_ctx(</span>
        <span class="c1">#         cp_mesh=parallel_dims.world_mesh[&quot;cp&quot;],</span>
        <span class="c1">#         cp_buffers=[inputs, targets] + [m.freqs_cis for m in model_parts],</span>
        <span class="c1">#         cp_seq_dims=[1, 1] + [0 for _ in model_parts],</span>
        <span class="c1">#         cp_no_restore_buffers={inputs, targets},</span>
        <span class="c1">#         cp_rotate_method=self.job_config.parallelism.context_parallel_rotate_method,</span>
        <span class="c1">#     )</span>
        <span class="c1">#     if parallel_dims.cp_enabled</span>
        <span class="c1">#     else None</span>
        <span class="c1"># )</span>
        <span class="n">optional_context_parallel_ctx</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">parallel_dims</span><span class="o">.</span><span class="n">pp_enabled</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;PP not implemented yet&quot;</span><span class="p">)</span>
            <span class="c1"># TODO implement PP</span>
            <span class="c1"># # Pipeline Parallel forward / backward inside step() call</span>
            <span class="c1"># with self.train_context(optional_context_parallel_ctx):</span>
            <span class="c1">#     targets, losses = (</span>
            <span class="c1">#         (labels, []) if self.pp_has_last_stage else (None, None)</span>
            <span class="c1">#     )</span>
            <span class="c1">#     if self.pp_has_first_stage:</span>
            <span class="c1">#         self.pp_schedule.step(</span>
            <span class="c1">#             inputs, target=targets, losses=losses, input_batch=inputs</span>
            <span class="c1">#         )</span>
            <span class="c1">#     else:</span>
            <span class="c1">#         self.pp_schedule.step(</span>
            <span class="c1">#             target=targets, losses=losses, input_batch=inputs</span>
            <span class="c1">#         )</span>
            <span class="c1">#</span>
            <span class="c1"># # accumulate losses across pipeline microbatches</span>
            <span class="c1"># # TODO: PP+FSDP unexpectedly puts the loss back to the CPU</span>
            <span class="c1"># loss = (</span>
            <span class="c1">#     torch.mean(torch.stack(losses)).to(self.device)</span>
            <span class="c1">#     if self.pp_has_last_stage</span>
            <span class="c1">#     else torch.tensor([-1.0], device=self.device)</span>
            <span class="c1"># )</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Non-PP forward / backward</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">train_context</span><span class="p">(</span><span class="n">optional_context_parallel_ctx</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_parts</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">maybe_enable_amp</span><span class="p">:</span>
                    <span class="n">logits</span> <span class="o">=</span> <span class="n">model_parts</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="o">**</span><span class="n">targets</span><span class="p">)</span>
                <span class="c1"># need to free to before bwd to avoid peaking memory</span>
                <span class="k">del</span> <span class="n">logits</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">loss</span></div>


    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">train_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]],</span> <span class="n">targets</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>

        <span class="c1"># Log timesteps</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">Tracer</span><span class="p">(</span><span class="s2">&quot;rl_trainer_perf/step&quot;</span><span class="p">,</span> <span class="n">timer</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">track_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">gc_handler</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">)</span>
        <span class="n">local_inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">dp_rank</span><span class="p">]</span>
        <span class="n">local_targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">dp_rank</span><span class="p">]</span>
        <span class="n">batch_to_device</span><span class="p">(</span><span class="n">local_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch_to_device</span><span class="p">(</span><span class="n">local_targets</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># compute policy logprobs</span>
        <span class="c1"># TODO implement gradient accumulation</span>
        <span class="c1"># with GradientAccumulation(</span>
        <span class="c1">#     self.gradient_accumulation_steps,</span>
        <span class="c1">#     self.model,</span>
        <span class="c1">#     self.data_parallel_size,</span>
        <span class="c1"># ) as grad_acc:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_backward</span><span class="p">(</span><span class="n">local_inputs</span><span class="p">,</span> <span class="n">local_targets</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;forward_backward&quot;</span><span class="p">)</span>

        <span class="c1"># Get learning rate from scheduler</span>
        <span class="n">current_lr</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">lr_schedulers</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">lr_schedulers</span><span class="p">,</span> <span class="s2">&quot;get_last_lr&quot;</span><span class="p">)</span>
            <span class="k">else</span> <span class="mf">0.001</span>
        <span class="p">)</span>
        <span class="n">record_metric</span><span class="p">(</span><span class="s2">&quot;rl_trainer/learning_rate&quot;</span><span class="p">,</span> <span class="n">current_lr</span><span class="p">,</span> <span class="n">Reduce</span><span class="o">.</span><span class="n">MIN</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">lr_schedulers</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;optimizer_step&quot;</span><span class="p">)</span>

        <span class="c1"># Record training metrics</span>
        <span class="c1"># TODO: delete item() to avoid cpu-gpu sync</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">record_metric</span><span class="p">(</span><span class="s2">&quot;rl_trainer/count_training_steps&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Reduce</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="n">record_metric</span><span class="p">(</span><span class="s2">&quot;rl_trainer/avg_grpo_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">Reduce</span><span class="o">.</span><span class="n">MEAN</span><span class="p">)</span>

        <span class="c1"># TODO: Extract actual KL divergence and policy entropy from the loss computation</span>
        <span class="c1"># These are placeholder values until the loss function exposes these metrics</span>
        <span class="c1"># record_metric(&quot;rl_trainer/step/avg_kl_divergence&quot;, 0.0, Reduce.MEAN)</span>
        <span class="c1"># record_metric(&quot;rl_trainer/step/std_kl_divergence&quot;, 0.0, Reduce.STD)</span>
        <span class="c1"># record_metric(&quot;rl_trainer/step/avg_policy_entropy&quot;, 0.0, Reduce.MEAN)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
            <span class="n">curr_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span>
            <span class="n">last_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_training_steps</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;save_checkpoint&quot;</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">push_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy_version</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Push weights to torchstore in HF format.&quot;&quot;&quot;</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">Tracer</span><span class="p">(</span><span class="s2">&quot;rl_trainer_perf/push_weights&quot;</span><span class="p">,</span> <span class="n">timer</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">track_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pushing weights for policy version </span><span class="si">{</span><span class="n">policy_version</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;model&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">states</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Model state not found in checkpointer state&quot;</span><span class="p">)</span>

        <span class="n">sd</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">flattened_state_dict</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">flatten_state_dict</span><span class="p">(</span><span class="n">sd</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;flatten_state_dict&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">sd_adapter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Trying to save checkpoint in HF safetensors format, but sd_adapter is not provided.&quot;</span>
            <span class="p">)</span>
        <span class="n">hf_state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">sd_adapter</span><span class="o">.</span><span class="n">to_hf</span><span class="p">(</span><span class="n">flattened_state_dict</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;to_hf&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_dcp</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">get_dcp_whole_state_dict_key</span><span class="p">(</span><span class="n">policy_version</span><span class="p">)</span>
            <span class="n">dcp_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dcp_path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">storage_writer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">FileSystemWriter</span><span class="p">(</span>
                <span class="n">dcp_id</span><span class="p">,</span> <span class="n">single_file_per_rank</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">thread_count</span><span class="o">=</span><span class="mi">8</span>
            <span class="p">)</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="n">dcp</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">storage_writer</span><span class="o">=</span><span class="n">storage_writer</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="n">hf_state_dict</span><span class="p">)</span>
            <span class="n">dcp_handle</span> <span class="o">=</span> <span class="n">DcpHandle</span><span class="p">(</span>
                <span class="n">checkpoint_id</span><span class="o">=</span><span class="n">dcp_id</span><span class="p">,</span>
                <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
                <span class="n">param_names</span><span class="o">=</span><span class="n">hf_state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span>
            <span class="p">)</span>
            <span class="k">await</span> <span class="n">ts</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">dcp_handle</span><span class="p">)</span>
            <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;dcp_save&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">hf_state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">key</span> <span class="o">=</span> <span class="n">get_param_key</span><span class="p">(</span><span class="n">policy_version</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                <span class="k">await</span> <span class="n">ts</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
            <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;ts_save&quot;</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Completed weights push in </span><span class="si">%.2f</span><span class="s2"> seconds&quot;</span><span class="p">,</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">cleanup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_shard_and_concat</span><span class="p">(</span><span class="n">sources</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">tp</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Shard and concatenate tensors along a given dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        source (list[torch.Tensor]): List of tensors to shard and concatenate.</span>
<span class="sd">        dim (int): Dimension along which to shard and concatenate.</span>
<span class="sd">        tp (int): Number of tensor parallel groups.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Concatenated tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sharded_sources</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">source</span> <span class="ow">in</span> <span class="n">sources</span><span class="p">:</span>
        <span class="n">sharded_sources</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">tp</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">))</span>

    <span class="n">combined_shards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">shard_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tp</span><span class="p">):</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="n">shard_idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sharded_sources</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">combined_shards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">combined_shards</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_qwen3_hf_to_vllm</span><span class="p">(</span>
    <span class="n">sd</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">vllm_tp</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert transformers state dict to vLLM format. Specifically, this fuses</span>
<span class="sd">    QKV projection and MLP gate_up_proj layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        sd (dict): State dict from HF model.</span>
<span class="sd">        num_layers (int): Number of layers in the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: State dict in vLLM format.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">load_sd</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">unwrap</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Unwrap a DTensor to a Tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">full_tensor</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">DTensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">t</span>

    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">sd</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">sd</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">unwrap</span><span class="p">(</span><span class="n">sd</span><span class="p">[</span><span class="n">key</span><span class="p">])</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

    <span class="c1"># Copy over directly mapped keys</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">sd</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
            <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span>
                <span class="s2">&quot;down_proj&quot;</span><span class="p">,</span>
                <span class="s2">&quot;input_layernorm&quot;</span><span class="p">,</span>
                <span class="s2">&quot;post_attention_layernorm&quot;</span><span class="p">,</span>
                <span class="s2">&quot;o_proj&quot;</span><span class="p">,</span>
                <span class="s2">&quot;norm.weight&quot;</span><span class="p">,</span>
                <span class="s2">&quot;embed_tokens.weight&quot;</span><span class="p">,</span>
                <span class="s2">&quot;lm_head.weight&quot;</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">):</span>
            <span class="n">load_sd</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="c1"># QKV fusion</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;self_attn.q_proj.weight&quot;</span><span class="p">]</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;self_attn.k_proj.weight&quot;</span><span class="p">]</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;self_attn.v_proj.weight&quot;</span><span class="p">]</span>

        <span class="n">load_sd</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;self_attn.qkv_proj.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_shard_and_concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tp</span><span class="o">=</span><span class="n">vllm_tp</span>
        <span class="p">)</span>

        <span class="c1"># Untested: QKV fusion - handle bias if present</span>
        <span class="n">q_bias_key</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;self_attn.q_proj.bias&quot;</span>
        <span class="n">k_bias_key</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;self_attn.k_proj.bias&quot;</span>
        <span class="n">v_bias_key</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;self_attn.v_proj.bias&quot;</span>

        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">key</span> <span class="ow">in</span> <span class="n">sd</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="n">q_bias_key</span><span class="p">,</span> <span class="n">k_bias_key</span><span class="p">,</span> <span class="n">v_bias_key</span><span class="p">]):</span>
            <span class="n">q_bias</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="n">q_bias_key</span><span class="p">]</span>
            <span class="n">k_bias</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="n">k_bias_key</span><span class="p">]</span>
            <span class="n">v_bias</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="n">v_bias_key</span><span class="p">]</span>
            <span class="n">load_sd</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;self_attn.qkv_proj.bias&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_shard_and_concat</span><span class="p">(</span>
                <span class="p">[</span><span class="n">q_bias</span><span class="p">,</span> <span class="n">k_bias</span><span class="p">,</span> <span class="n">v_bias</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tp</span><span class="o">=</span><span class="n">vllm_tp</span>
            <span class="p">)</span>

        <span class="c1"># MLP gate_up_proj fusion</span>
        <span class="n">gate</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;mlp.gate_proj.weight&quot;</span><span class="p">]</span>
        <span class="n">up</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;mlp.up_proj.weight&quot;</span><span class="p">]</span>
        <span class="n">load_sd</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;mlp.gate_up_proj.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_shard_and_concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">gate</span><span class="p">,</span> <span class="n">up</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tp</span><span class="o">=</span><span class="n">vllm_tp</span>
        <span class="p">)</span>

        <span class="c1"># Untested: MLP gate_up_proj fusion - handle bias if present</span>
        <span class="n">gate_bias_key</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;mlp.gate_proj.bias&quot;</span>
        <span class="n">up_bias_key</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;mlp.up_proj.bias&quot;</span>

        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">key</span> <span class="ow">in</span> <span class="n">sd</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="n">gate_bias_key</span><span class="p">,</span> <span class="n">up_bias_key</span><span class="p">]):</span>
            <span class="n">gate_bias</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="n">gate_bias_key</span><span class="p">]</span>
            <span class="n">up_bias</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="n">up_bias_key</span><span class="p">]</span>
            <span class="c1"># Same sharding has to happen here</span>
            <span class="n">load_sd</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;mlp.gate_up_proj.bias&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_shard_and_concat</span><span class="p">(</span>
                <span class="p">[</span><span class="n">gate_bias</span><span class="p">,</span> <span class="n">up_bias</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tp</span><span class="o">=</span><span class="n">vllm_tp</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">load_sd</span>
</pre></div>

                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  


<style>
.site-footer {
    padding: 20px 40px;
    height: 60px !important;
}

@media screen and (min-width: 768px) {
    .site-footer {
        padding: 20px 40px;
    }
}

.site-footer .privacy-policy {
    border-top: none;
    margin-top: 0px;
}

.site-footer .privacy-policy .copyright {
    padding-top: 0;
}
</style>


<footer class="site-footer">

    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
           Copyright © 2025 Meta Platforms, Inc
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "forge.actors.trainer",
       "headline": "forge.actors.trainer",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/_modules/forge/actors/trainer.html",
       "articleBody": "Source code for forge.actors.trainer # Copyright (c) Meta Platforms, Inc. and affiliates. # All rights reserved. # # This source code is licensed under the BSD-style license found in the # LICENSE file in the root directory of this source tree. import logging import os import shutil import time from collections.abc import Mapping from dataclasses import dataclass, field, fields from typing import Callable import torch import torch.distributed.checkpoint as dcp import torchstore as ts from monarch.actor import endpoint from torch import Tensor from torch.distributed.checkpoint._nested_dict import flatten_state_dict from torchtitan.config.job_config import ( ActivationCheckpoint, Checkpoint, Comm, Compile, Job, LRScheduler, MemoryEstimation, Model, Optimizer, Parallelism, Quantize, Training, ) from torchtitan.experiments.forge.engine import ForgeEngine from torchtitan.experiments.forge.job_config import ForgeJobConfig from forge.actors._torchstore_utils import ( DcpHandle, get_dcp_whole_state_dict_key, get_param_key, ) from forge.controller import ForgeActor from forge.data.utils import batch_to_device from forge.env import TORCHSTORE_USE_RDMA from forge.observability.metrics import record_metric, Reduce from forge.observability.perf_tracker import Tracer logger = logging.getLogger(__name__) logger.setLevel(logging.DEBUG) def cleanup_old_weight_versions( state_dict_key: str, delim: str, current_policy_version: int, ) -\u003e None: \"\"\"Delete old weight versions, keeping only current and N-1 versions. TODO - issues/194: provide a more robust way to handle eviction. Args: state_dict_key: The base key for state dict storage delim: The delimiter used between key and version current_policy_version: The current policy version to keep \"\"\" if current_policy_version \u003c= 1: return # No cleanup needed for versions 0 or 1 prefix = f\"{state_dict_key}{delim}\" current_weights = f\"{prefix}{current_policy_version}\" previous_weights = f\"{prefix}{current_policy_version - 1}\" # Find all weight directories that match our pattern parent_dir = os.path.dirname(prefix) or \".\" if os.path.exists(parent_dir): for item in os.listdir(parent_dir): item_path = os.path.join(parent_dir, item) if ( item.startswith(os.path.basename(prefix)) and item != os.path.basename(current_weights) and item != os.path.basename(previous_weights) and os.path.isdir(item_path) ): try: shutil.rmtree(item_path, ignore_errors=True) logger.debug(f\"Removed old weights at {item_path}\") except OSError as e: logger.debug(f\"Error deleting {item_path}: {e}\") [docs] @dataclass class RLTrainer(ForgeActor): \"\"\"A reinforcement learning trainer actor for policy optimization training. Built on top of TorchTitan\u0027s training engine, this actor provides a complete training loop for reinforcement learning. It performs forward and backward passes with gradient computation, optimization steps, and checkpoint management. Unlike the ReferenceModel actor which only runs forward passes, RLTrainer actively updates the policy model parameters through gradient descent. The trainer supports the same distributed training strategies that TorchTitan does, including but not limited to, tensor parallelism, data parallelism, and FSDP (Fully Sharded Data Parallel). It is typically used in conjunction with ReferenceModel for policy optimization algorithms like GRPO (Group Relative Policy Optimization), where it optimizes the policy against a loss that includes KL divergence penalties from the reference model. The trainer handles: - Forward and backward propagation with automatic mixed precision (AMP) - Optimizer steps with learning rate scheduling \"\"\" job: Job = field(default_factory=Job) model: Model = field(default_factory=Model) optimizer: Optimizer = field(default_factory=Optimizer) lr_scheduler: LRScheduler = field(default_factory=LRScheduler) training: Training = field(default_factory=Training) parallelism: Parallelism = field(default_factory=Parallelism) checkpoint: Checkpoint = field(default_factory=Checkpoint) activation_checkpoint: ActivationCheckpoint = field( default_factory=ActivationCheckpoint ) compile: Compile = field(default_factory=Compile) quantize: Quantize = field(default_factory=Quantize) comm: Comm = field(default_factory=Comm) memory_estimation: MemoryEstimation = field(default_factory=MemoryEstimation) # Non JobConfig-related fields loss: Callable = lambda logits, **targets: logits state_dict_key: str = \"model_state_dict\" use_dcp: bool = ( TORCHSTORE_USE_RDMA.get_value() == 0 ) # torchstore currently only accepts 0 or 1 dcp_path: str = \"forge_dcp_tmp\" def __post_init__(self): \"\"\"Initializes config types and env variables. torchrun normally hands env variables, but we need to do it ourselves in monarch for now. \"\"\" super().__init__() if self.use_dcp: # DCP specific optimization torch.serialization.set_crc32_options(False) # Instantiate dict fields for f in fields(self): attr = getattr(self, f.name) if isinstance(attr, Mapping): setattr(self, f.name, f.type(**attr)) elif not isinstance(attr, f.type): raise TypeError( f\"{f.name} should be a {f.type} type or a dict like object\" ) self.step = 1 # fragile contract. self.num_training_steps = self.training.steps self.gradient_accumulation_steps = 1 os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\" logger.info(\"Compiling loss\") self.loss = torch.compile(self.loss) @endpoint async def setup(self): # TODO: update ForgeEngine to not use ForgeJobConfig engine_config = {f.name: getattr(self, f.name) for f in fields(self)} for key in { \"loss\", \"state_dict_key\", \"use_dcp\", \"dcp_path\", }: engine_config.pop(key) # Not part of job config self.engine = ForgeEngine(ForgeJobConfig(**engine_config)) self.engine.checkpointer.load(step=self.step) self.engine.optimizers.zero_grad() [docs] def forward_backward( self, inputs: dict[str, Tensor], targets: dict[str, Tensor] ) -\u003e Tensor: model_parts = self.engine.model_parts parallel_dims = self.engine.parallel_dims # apply context parallelism if cp is enabled # ensure CP handles the separate freqs_cis buffer for each pp stage # if getattr(self.engine.model_args, \"use_flex_attn\", False): # cp_mesh = ( # parallel_dims.world_mesh[\"cp\"] if parallel_dims.cp_enabled else None # ) # init_attention_mask( # inputs, self.engine.tokenizer.base_tokenizer.eos_id, cp_mesh # ) # optional_context_parallel_ctx = ( # dist_utils.create_context_parallel_ctx( # cp_mesh=parallel_dims.world_mesh[\"cp\"], # cp_buffers=[inputs, targets] + [m.freqs_cis for m in model_parts], # cp_seq_dims=[1, 1] + [0 for _ in model_parts], # cp_no_restore_buffers={inputs, targets}, # cp_rotate_method=self.job_config.parallelism.context_parallel_rotate_method, # ) # if parallel_dims.cp_enabled # else None # ) optional_context_parallel_ctx = None if parallel_dims.pp_enabled: raise NotImplementedError(\"PP not implemented yet\") # TODO implement PP # # Pipeline Parallel forward / backward inside step() call # with self.train_context(optional_context_parallel_ctx): # targets, losses = ( # (labels, []) if self.pp_has_last_stage else (None, None) # ) # if self.pp_has_first_stage: # self.pp_schedule.step( # inputs, target=targets, losses=losses, input_batch=inputs # ) # else: # self.pp_schedule.step( # target=targets, losses=losses, input_batch=inputs # ) # # # accumulate losses across pipeline microbatches # # TODO: PP+FSDP unexpectedly puts the loss back to the CPU # loss = ( # torch.mean(torch.stack(losses)).to(self.device) # if self.pp_has_last_stage # else torch.tensor([-1.0], device=self.device) # ) else: # Non-PP forward / backward with self.engine.train_context(optional_context_parallel_ctx): assert len(model_parts) == 1 with self.engine.maybe_enable_amp: logits = model_parts[0](**inputs) loss = self.loss(logits, **targets) # need to free to before bwd to avoid peaking memory del logits loss.backward() return loss @endpoint async def train_step( self, inputs: list[dict[str, Tensor]], targets: list[dict[str, Tensor]] ) -\u003e float: # Log timesteps t = Tracer(\"rl_trainer_perf/step\", timer=\"gpu\", track_memory=True) t.start() self.engine.gc_handler.run(self.step) local_inputs = inputs[self.engine.dp_rank] local_targets = targets[self.engine.dp_rank] batch_to_device(local_inputs, self.engine.device) batch_to_device(local_targets, self.engine.device) # compute policy logprobs # TODO implement gradient accumulation # with GradientAccumulation( # self.gradient_accumulation_steps, # self.model, # self.data_parallel_size, # ) as grad_acc: loss = self.forward_backward(local_inputs, local_targets) torch.distributed.all_reduce(loss) t.step(\"forward_backward\") # Get learning rate from scheduler current_lr = ( self.engine.lr_schedulers.get_last_lr()[0] if hasattr(self.engine.lr_schedulers, \"get_last_lr\") else 0.001 ) record_metric(\"rl_trainer/learning_rate\", current_lr, Reduce.MIN) self.engine.optimizers.step() self.engine.optimizers.zero_grad() self.engine.lr_schedulers.step() t.step(\"optimizer_step\") # Record training metrics # TODO: delete item() to avoid cpu-gpu sync loss = loss.detach().cpu().item() record_metric(\"rl_trainer/count_training_steps\", 1, Reduce.SUM) record_metric(\"rl_trainer/avg_grpo_loss\", loss, Reduce.MEAN) # TODO: Extract actual KL divergence and policy entropy from the loss computation # These are placeholder values until the loss function exposes these metrics # record_metric(\"rl_trainer/step/avg_kl_divergence\", 0.0, Reduce.MEAN) # record_metric(\"rl_trainer/step/std_kl_divergence\", 0.0, Reduce.STD) # record_metric(\"rl_trainer/step/avg_policy_entropy\", 0.0, Reduce.MEAN) self.step += 1 self.engine.checkpointer.save( curr_step=self.step, last_step=self.step == self.num_training_steps, ) t.step(\"save_checkpoint\") t.stop() return loss @endpoint async def push_weights(self, policy_version: int) -\u003e None: \"\"\"Push weights to torchstore in HF format.\"\"\" t = Tracer(\"rl_trainer_perf/push_weights\", timer=\"gpu\", track_memory=True) t.start() logger.info(f\"Pushing weights for policy version {policy_version}\") start_time = time.perf_counter() if \"model\" not in self.engine.checkpointer.states: raise RuntimeError(\"Model state not found in checkpointer state\") sd = self.engine.checkpointer.states[\"model\"].state_dict() flattened_state_dict, _ = flatten_state_dict(sd) t.step(\"flatten_state_dict\") if self.engine.checkpointer.sd_adapter is None: raise RuntimeError( \"Trying to save checkpoint in HF safetensors format, but sd_adapter is not provided.\" ) hf_state_dict = self.engine.checkpointer.sd_adapter.to_hf(flattened_state_dict) t.step(\"to_hf\") if self.use_dcp: key = get_dcp_whole_state_dict_key(policy_version) dcp_id = f\"{self.dcp_path}/{key}\" storage_writer = torch.distributed.checkpoint.FileSystemWriter( dcp_id, single_file_per_rank=False, thread_count=8 ) metadata = dcp.save(storage_writer=storage_writer, state_dict=hf_state_dict) dcp_handle = DcpHandle( checkpoint_id=dcp_id, metadata=metadata, param_names=hf_state_dict.keys(), ) await ts.put(key, dcp_handle) t.step(\"dcp_save\") else: for name, param in hf_state_dict.items(): key = get_param_key(policy_version, name) await ts.put(key, param) t.step(\"ts_save\") t.stop() end_time = time.perf_counter() logger.info(\"Completed weights push in %.2f seconds\", end_time - start_time) @endpoint async def cleanup(self) -\u003e None: if self.engine.checkpointer: self.engine.checkpointer.close() def _shard_and_concat(sources: list[torch.Tensor], dim: int, tp: int) -\u003e torch.Tensor: \"\"\"Shard and concatenate tensors along a given dimension. Args: source (list[torch.Tensor]): List of tensors to shard and concatenate. dim (int): Dimension along which to shard and concatenate. tp (int): Number of tensor parallel groups. Returns: torch.Tensor: Concatenated tensor. \"\"\" sharded_sources = [] for source in sources: sharded_sources.append(torch.chunk(source, tp, dim=dim)) combined_shards = [] for shard_idx in range(tp): combined = torch.cat([s[shard_idx] for s in sharded_sources], dim=dim) combined_shards.append(combined) return torch.cat(combined_shards, dim=dim) def _qwen3_hf_to_vllm( sd: dict[str, torch.Tensor], num_layers: int, vllm_tp: int ) -\u003e dict[str, torch.Tensor]: \"\"\"Convert transformers state dict to vLLM format. Specifically, this fuses QKV projection and MLP gate_up_proj layers. Args: sd (dict): State dict from HF model. num_layers (int): Number of layers in the model. Returns: dict: State dict in vLLM format. \"\"\" load_sd = {} def unwrap(t): \"\"\"Unwrap a DTensor to a Tensor.\"\"\" return t.full_tensor() if isinstance(t, torch.distributed.tensor.DTensor) else t for key in sd.keys(): sd[key] = unwrap(sd[key]).cpu() # Copy over directly mapped keys for k in sd: if any( x in k for x in [ \"down_proj\", \"input_layernorm\", \"post_attention_layernorm\", \"o_proj\", \"norm.weight\", \"embed_tokens.weight\", \"lm_head.weight\", ] ): load_sd[k] = sd[k] for i in range(num_layers): prefix = f\"model.layers.{i}.\" # QKV fusion q = sd[prefix + \"self_attn.q_proj.weight\"] k = sd[prefix + \"self_attn.k_proj.weight\"] v = sd[prefix + \"self_attn.v_proj.weight\"] load_sd[prefix + \"self_attn.qkv_proj.weight\"] = _shard_and_concat( [q, k, v], dim=0, tp=vllm_tp ) # Untested: QKV fusion - handle bias if present q_bias_key = prefix + \"self_attn.q_proj.bias\" k_bias_key = prefix + \"self_attn.k_proj.bias\" v_bias_key = prefix + \"self_attn.v_proj.bias\" if all(key in sd for key in [q_bias_key, k_bias_key, v_bias_key]): q_bias = sd[q_bias_key] k_bias = sd[k_bias_key] v_bias = sd[v_bias_key] load_sd[prefix + \"self_attn.qkv_proj.bias\"] = _shard_and_concat( [q_bias, k_bias, v_bias], dim=0, tp=vllm_tp ) # MLP gate_up_proj fusion gate = sd[prefix + \"mlp.gate_proj.weight\"] up = sd[prefix + \"mlp.up_proj.weight\"] load_sd[prefix + \"mlp.gate_up_proj.weight\"] = _shard_and_concat( [gate, up], dim=0, tp=vllm_tp ) # Untested: MLP gate_up_proj fusion - handle bias if present gate_bias_key = prefix + \"mlp.gate_proj.bias\" up_bias_key = prefix + \"mlp.up_proj.bias\" if all(key in sd for key in [gate_bias_key, up_bias_key]): gate_bias = sd[gate_bias_key] up_bias = sd[up_bias_key] # Same sharding has to happen here load_sd[prefix + \"mlp.gate_up_proj.bias\"] = _shard_and_concat( [gate_bias, up_bias], dim=0, tp=vllm_tp ) return load_sd",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../../../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/_modules/forge/actors/trainer.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>