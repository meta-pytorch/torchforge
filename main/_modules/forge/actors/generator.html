
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>forge.actors.generator &#8212; torchforge 0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=b61afe48" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=2709fde1"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">
import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs';
mermaid.initialize({
    startOnLoad: false,
    theme: 'base',
    themeVariables: {
        primaryColor: '#4CAF50',
        primaryTextColor: '#000',
        primaryBorderColor: '#fff',
        lineColor: '#555',
        secondaryColor: '#FF9800',
        tertiaryColor: '#ffffde'
    },
    flowchart: {
        curve: 'basis'
    },
    themeCSS: '.edgePath .path { stroke-width: 4px; stroke: #555; }'
});
</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/forge/actors/generator';</script>
    <script src="../../../_static/custom.js?v=0065d487"></script>
    <link rel="canonical" href="https://meta-pytorch.org/forge/main/_modules/forge/actors/generator.html" />
    <link rel="icon" href="../../../_static/logo-icon.svg"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
</style>


<style>
  @media (min-width: 1100px) {
    .site-footer {
      height: 300px !important;
    }
  }
</style>

<link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../../../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NPLPKN5G" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-NPLPKN5G');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', '');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<script>
  // Define repository configuration for tutorial buttons using existing html_context variables
  // Only injected when tutorial buttons are shown AND github variables are defined
  // If either condition is false, JavaScript will fallback to default PyTorch tutorial links
  window.repoConfig = {
    github_repo: "meta-pytorch/forge",
    github_branch: "main",
    colab_repo: "meta-pytorch/forge",
    colab_branch: "gh-pages"
  };
</script>

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/meta-pytorch/forge" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../getting_started.html">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/forge" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchforge/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../getting_started.html">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../tutorials.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../api.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/forge" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchforge/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">forge.actors...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../../index.html">
        <meta itemprop="name" content="Module code">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="forge.actors.generator">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">_modules/forge/actors/generator</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../../../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../../../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../../../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for forge.actors.generator</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD-style license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Mapping</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">copy</span><span class="w"> </span><span class="kn">import</span> <span class="n">copy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchstore</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ts</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">monarch.actor</span><span class="w"> </span><span class="kn">import</span> <span class="n">current_rank</span><span class="p">,</span> <span class="n">endpoint</span><span class="p">,</span> <span class="n">ProcMesh</span><span class="p">,</span> <span class="n">this_host</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">VllmConfig</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.engine.arg_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">EngineArgs</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.entrypoints.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_validate_truncation_size</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.executor.multiproc_worker_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">set_multiprocessing_worker_envs</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.outputs</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompletionOutput</span><span class="p">,</span> <span class="n">RequestOutput</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.sampling_params</span><span class="w"> </span><span class="kn">import</span> <span class="n">RequestOutputKind</span><span class="p">,</span> <span class="n">SamplingParams</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.transformers_utils.tokenizer_group</span><span class="w"> </span><span class="kn">import</span> <span class="n">init_tokenizer_from_configs</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.usage.usage_lib</span><span class="w"> </span><span class="kn">import</span> <span class="n">UsageContext</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_distributed_init_method</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.v1.core.kv_cache_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_kv_cache_config</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.v1.core.sched.output</span><span class="w"> </span><span class="kn">import</span> <span class="n">SchedulerOutput</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.v1.core.sched.scheduler</span><span class="w"> </span><span class="kn">import</span> <span class="n">Scheduler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.v1.engine</span><span class="w"> </span><span class="kn">import</span> <span class="n">EngineCoreOutputs</span><span class="p">,</span> <span class="n">EngineCoreRequest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.v1.engine.output_processor</span><span class="w"> </span><span class="kn">import</span> <span class="n">OutputProcessor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.v1.engine.parallel_sampling</span><span class="w"> </span><span class="kn">import</span> <span class="n">ParentRequest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.v1.engine.processor</span><span class="w"> </span><span class="kn">import</span> <span class="n">Processor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.v1.kv_cache_interface</span><span class="w"> </span><span class="kn">import</span> <span class="n">KVCacheConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.v1.outputs</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelRunnerOutput</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.v1.request</span><span class="w"> </span><span class="kn">import</span> <span class="n">Request</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.v1.structured_output</span><span class="w"> </span><span class="kn">import</span> <span class="n">StructuredOutputManager</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.worker.worker_base</span><span class="w"> </span><span class="kn">import</span> <span class="n">WorkerWrapperBase</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">forge.actors._torchstore_utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">extract_param_name</span><span class="p">,</span>
    <span class="n">get_dcp_whole_state_dict_key</span><span class="p">,</span>
    <span class="n">get_param_key</span><span class="p">,</span>
    <span class="n">get_param_prefix</span><span class="p">,</span>
    <span class="n">load_tensor_from_dcp</span><span class="p">,</span>
    <span class="n">rdma_available</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">forge.controller</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">ForgeActor</span><span class="p">,</span>
    <span class="n">get_proc_mesh</span><span class="p">,</span>
    <span class="n">host_mesh_from_proc</span><span class="p">,</span>
    <span class="n">stop_proc_mesh</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">forge.data_models.completion</span><span class="w"> </span><span class="kn">import</span> <span class="n">Completion</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">forge.data_models.prompt</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_prompt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">forge.observability.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">record_metric</span><span class="p">,</span> <span class="n">Reduce</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">forge.observability.perf_tracker</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tracer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">forge.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">ProcessConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">forge.util._shared_tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">SharedTensor</span><span class="p">,</span> <span class="n">SharedTensorHandle</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>


<div class="viewcode-block" id="Generator">
<a class="viewcode-back" href="../../../api_generator.html#forge.actors.generator.Generator">[docs]</a>
<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Generator</span><span class="p">(</span><span class="n">ForgeActor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Instance of a vLLM-based generator.</span>

<span class="sd">    This class manually recreates a vLLM engine that mirrors the design of AsyncLLMEngine in v1. The</span>
<span class="sd">    main difference is that all communications are controlled here via Monarch&#39;s proc meshes.</span>

<span class="sd">    Args:</span>
<span class="sd">        engine_args (EngineArgs): The engine arguments to use for the vLLM engine.</span>
<span class="sd">        sampling_params (SamplingParams): The sampling parameters to use for the vLLM engine.</span>
<span class="sd">        use_dcp_for_weight_sync (bool): Whether to use DCP for NFS-based weight sync. Default depends on</span>
<span class="sd">            whether or not RDMA is enabled in torchstore. If it is, then DCP is disabled. Otherwise, DCP is enabled.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; generator = await Generator.options(procs=1, num_replicas=1, with_gpus=True).as_service(</span>
<span class="sd">    ...     engine_args=EngineArgs(...),</span>
<span class="sd">    ...     sampling_params=SamplingParams(...),</span>
<span class="sd">    ...     )</span>
<span class="sd">    &gt;&gt;&gt; await generator.generate(&quot;Tell me a joke&quot;)</span>
<span class="sd">    Completion(prompt=&quot;Tell me a joke&quot;, text=&quot;A: Why did the chicken cross the road? B: To get to the other side.&quot;,</span>
<span class="sd">    token_ids=[...], logprobs=[...])</span>
<span class="sd">    &gt;&gt;&gt; await generator.shutdown()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">engine_args</span><span class="p">:</span> <span class="n">EngineArgs</span> <span class="o">|</span> <span class="n">Mapping</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">EngineArgs</span><span class="p">)</span>
    <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span> <span class="o">|</span> <span class="n">Mapping</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">SamplingParams</span><span class="p">)</span>
    <span class="n">use_dcp_for_weight_sync</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">prefetch_weights_to_shm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">n_fetcher_procs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_run_task</span><span class="p">:</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Task</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_generator_proc</span><span class="p">:</span> <span class="n">ProcMesh</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_worker_procs</span><span class="p">:</span> <span class="n">ProcMesh</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="p">:</span> <span class="n">GeneratorWorker</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator_version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">engine_args</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">engine_args</span> <span class="o">=</span> <span class="n">EngineArgs</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">engine_args</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine_args</span><span class="o">.</span><span class="n">_is_v1_supported_oracle</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">_</span><span class="p">:</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine_args</span><span class="o">.</span><span class="n">create_engine_config</span><span class="p">(</span><span class="n">UsageContext</span><span class="o">.</span><span class="n">LLM_CLASS</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampling_params</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="o">.</span><span class="n">from_optional</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">sampling_params</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sampling_params</span><span class="o">.</span><span class="n">output_kind</span> <span class="o">=</span> <span class="n">RequestOutputKind</span><span class="o">.</span><span class="n">FINAL_ONLY</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_dcp_for_weight_sync</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_dcp_for_weight_sync</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">rdma_available</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">use_dcp_for_weight_sync</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">get_vllm_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">VllmConfig</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">register_worker</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">worker</span><span class="p">:</span> <span class="n">GeneratorWorker</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker</span> <span class="o">=</span> <span class="n">worker</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Registered GeneratorWorker on Generator.&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">launch</span><span class="p">(</span>  <span class="c1"># pyright: ignore[reportIncompatibleMethodOverride]</span>
        <span class="bp">cls</span><span class="p">:</span> <span class="nb">type</span><span class="p">[</span><span class="s2">&quot;Generator&quot;</span><span class="p">],</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Generator&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Custom launch for the Generator service with its workers.</span>

<span class="sd">        We overwrite the default Service launch method in order to setup Actors (GeneratorWorker) within this &quot;coordinating&quot; Actor.</span>
<span class="sd">        We first create a proc_mesh for the workers, then a proc_mesh for the generator, and then we spawn the workers</span>
<span class="sd">        and the generator in setup.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Note: get_proc_mesh will set MASTER_ADDR, MASTER_PORT and CUDA_VISIBLE_DEVICES</span>
        <span class="n">process_config</span><span class="p">:</span> <span class="n">ProcessConfig</span> <span class="o">=</span> <span class="n">ProcessConfig</span><span class="p">(</span>
            <span class="n">procs</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">procs</span><span class="p">,</span>
            <span class="n">hosts</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">hosts</span><span class="p">,</span>
            <span class="n">with_gpus</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">with_gpus</span><span class="p">,</span>
            <span class="n">mesh_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">mesh_name</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># First, spawn the worker processes which may or may not be</span>
        <span class="c1"># on remote hosts.</span>
        <span class="n">worker_procs</span> <span class="o">=</span> <span class="k">await</span> <span class="n">get_proc_mesh</span><span class="p">(</span><span class="n">process_config</span><span class="o">=</span><span class="n">process_config</span><span class="p">)</span>

        <span class="c1"># Then, grab a single host from the workers...</span>
        <span class="n">host_mesh</span> <span class="o">=</span> <span class="k">await</span> <span class="n">host_mesh_from_proc</span><span class="p">(</span><span class="n">worker_procs</span><span class="p">)</span>
        <span class="n">singleton_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">host_mesh</span><span class="o">.</span><span class="n">extent</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
        <span class="n">host_mesh</span> <span class="o">=</span> <span class="n">host_mesh</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="o">**</span><span class="n">singleton_slice</span><span class="p">)</span>

        <span class="c1"># We ask the provisioner for a single process on a single host</span>
        <span class="n">generator_proc_config</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">process_config</span><span class="p">)</span>
        <span class="n">generator_proc_config</span><span class="o">.</span><span class="n">procs</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">generator_proc_config</span><span class="o">.</span><span class="n">with_gpus</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">generator_proc</span> <span class="o">=</span> <span class="k">await</span> <span class="n">get_proc_mesh</span><span class="p">(</span>
            <span class="n">process_config</span><span class="o">=</span><span class="n">generator_proc_config</span><span class="p">,</span> <span class="n">host_mesh</span><span class="o">=</span><span class="n">host_mesh</span>
        <span class="p">)</span>
        <span class="c1"># TODO - expand support so name can stick within kwargs</span>
        <span class="n">actor_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
        <span class="n">generator</span> <span class="o">=</span> <span class="n">generator_proc</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span>
            <span class="n">actor_name</span><span class="p">,</span>
            <span class="bp">cls</span><span class="p">,</span>
            <span class="o">*</span><span class="n">args</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">vllm_config</span> <span class="o">=</span> <span class="p">(</span>
            <span class="k">await</span> <span class="n">generator</span><span class="o">.</span><span class="n">get_vllm_config</span><span class="o">.</span><span class="n">call_one</span><span class="p">()</span>
        <span class="p">)</span>  <span class="c1"># Config should be the same across all actors</span>
        <span class="n">worker</span> <span class="o">=</span> <span class="n">worker_procs</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span>
            <span class="s2">&quot;vllm_worker&quot;</span><span class="p">,</span> <span class="n">GeneratorWorker</span><span class="p">,</span> <span class="n">vllm_config</span><span class="o">=</span><span class="n">vllm_config</span>
        <span class="p">)</span>
        <span class="k">await</span> <span class="n">worker</span><span class="o">.</span><span class="n">setup</span><span class="o">.</span><span class="n">call</span><span class="p">()</span>
        <span class="k">await</span> <span class="n">generator</span><span class="o">.</span><span class="n">register_worker</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">worker</span><span class="p">)</span>

        <span class="n">generator</span><span class="o">.</span><span class="n">_generator_proc</span> <span class="o">=</span> <span class="n">generator_proc</span>
        <span class="n">generator</span><span class="o">.</span><span class="n">_worker_procs</span> <span class="o">=</span> <span class="n">worker_procs</span>
        <span class="k">await</span> <span class="n">generator</span><span class="o">.</span><span class="n">setup</span><span class="o">.</span><span class="n">call</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">generator</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Mirrors the __init__ of vLLM&#39;s LLMEngine.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">request_id</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">requests</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">ParentRequest</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Future</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># TODO: Investigate whether this can be combined with `generator.running`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accepting_requests</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">request_lock</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Condition</span><span class="p">()</span>  <span class="c1"># Guard for accepting_requests</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_lock</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Condition</span><span class="p">()</span>  <span class="c1"># Guard for updating requests</span>

        <span class="c1"># Setup processors</span>
        <span class="c1"># TODO: move all processing to the Environment</span>
        <span class="c1"># TODO: add support for `log_stats` and `mm_registry`</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">init_tokenizer_from_configs</span><span class="p">(</span>
            <span class="n">model_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="o">.</span><span class="n">model_config</span><span class="p">,</span>
            <span class="n">scheduler_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="o">.</span><span class="n">scheduler_config</span><span class="p">,</span>
            <span class="n">lora_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="o">.</span><span class="n">lora_config</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">processor</span> <span class="o">=</span> <span class="n">Processor</span><span class="p">(</span>
            <span class="n">vllm_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">mm_registry</span><span class="o">=</span><span class="kc">None</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_processor</span> <span class="o">=</span> <span class="n">OutputProcessor</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">log_stats</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Configure KV caches</span>
        <span class="n">kv_cache_configs</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">setup_kv_cache</span><span class="o">.</span><span class="n">call</span><span class="p">()</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">kv_cache_config</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">kv_cache_configs</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="o">.</span><span class="n">cache_config</span><span class="o">.</span><span class="n">num_gpu_blocks</span> <span class="o">=</span> <span class="n">kv_cache_config</span><span class="o">.</span><span class="n">num_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="o">.</span><span class="n">cache_config</span><span class="o">.</span><span class="n">num_cpu_blocks</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Setup scheduler</span>
        <span class="c1"># TODO: Add support for `log_stats`</span>
        <span class="n">structured_output_manager</span> <span class="o">=</span> <span class="n">StructuredOutputManager</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">Scheduler</span><span class="p">(</span>
            <span class="n">vllm_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="p">,</span>
            <span class="n">kv_cache_config</span><span class="o">=</span><span class="n">kv_cache_config</span><span class="p">,</span>
            <span class="n">structured_output_manager</span><span class="o">=</span><span class="n">structured_output_manager</span><span class="p">,</span>
            <span class="n">include_finished_set</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">log_stats</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_start_processing</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_weights_to_shm</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_spawn_fetchers</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_spawn_fetchers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Spawn weight fetchers that prefetch weights from torchstore to shared memory.&quot;&quot;&quot;</span>
        <span class="c1"># TODO: this assumes the generator is on the same host as the worker</span>
        <span class="c1"># and only works for single host generators. Figure out how to support</span>
        <span class="c1"># generators with workers spanned across multiple hosts.</span>
        <span class="n">fetcher_procs</span> <span class="o">=</span> <span class="n">this_host</span><span class="p">()</span><span class="o">.</span><span class="n">spawn_procs</span><span class="p">(</span>
            <span class="n">per_host</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;procs&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_fetcher_procs</span><span class="p">}</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fetcher_procs</span> <span class="o">=</span> <span class="n">fetcher_procs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_fetchers</span> <span class="o">=</span> <span class="n">fetcher_procs</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="s2">&quot;weight_fetcher&quot;</span><span class="p">,</span> <span class="n">_WeightFetcher</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_start_processing</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_task</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_task</span><span class="o">.</span><span class="n">done</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_run_task</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">create_task</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="p">())</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_drop_shared_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">SharedTensorHandle</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">drop</span><span class="p">()</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_fetch_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">version</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">SharedTensorHandle</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fetch weights from torchstore and return a dict of {name: SharedTensorHandle}.&quot;&quot;&quot;</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">Tracer</span><span class="p">(</span><span class="s2">&quot;generator_perf/_fetch_weights&quot;</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="n">get_param_prefix</span><span class="p">(</span><span class="n">version</span><span class="p">)</span>
        <span class="n">matching_keys</span> <span class="o">=</span> <span class="k">await</span> <span class="n">ts</span><span class="o">.</span><span class="n">keys</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span>
        <span class="n">hf_param_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">extract_param_name</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">matching_keys</span><span class="p">]</span>

        <span class="n">n_fetchers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_fetchers</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">split_keys</span><span class="p">(</span><span class="n">keys</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="n">n_fetchers</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_fetchers</span><span class="p">)]</span>

        <span class="n">futures</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">names</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">split_keys</span><span class="p">(</span><span class="n">hf_param_names</span><span class="p">)):</span>
            <span class="n">fut</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_fetchers</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">procs</span><span class="o">=</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">fetch</span><span class="o">.</span><span class="n">call_one</span><span class="p">(</span>
                <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">,</span> <span class="n">param_names</span><span class="o">=</span><span class="n">names</span>
            <span class="p">)</span>
            <span class="n">futures</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fut</span><span class="p">)</span>

        <span class="n">sub_state_dicts</span> <span class="o">=</span> <span class="p">[</span><span class="k">await</span> <span class="n">fut</span> <span class="k">for</span> <span class="n">fut</span> <span class="ow">in</span> <span class="n">futures</span><span class="p">]</span>

        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">sd</span> <span class="ow">in</span> <span class="n">sub_state_dicts</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">sd</span><span class="p">)</span>

        <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">priority</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Completion</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate a response for the given prompt</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (str): The prompt to generate a response for.</span>
<span class="sd">            priority (int, optional): The priority of the request. Defaults to 0.</span>

<span class="sd">        Returns:</span>
<span class="sd">            list[Completion]: n completions from vLLM based on your prompt.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">Tracer</span><span class="p">(</span><span class="s2">&quot;generator_perf/generate&quot;</span><span class="p">,</span> <span class="n">timer</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="n">record_metric</span><span class="p">(</span><span class="s2">&quot;generator/generate/count_requests&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Reduce</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">request_id</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">%</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span>
        <span class="n">request_id</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">request_id</span><span class="p">)</span>

        <span class="n">tokenization_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># TODO: add truncation support https://github.com/vllm-project/vllm/issues/4507</span>
        <span class="n">truncate_prompt_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampling_params</span><span class="o">.</span><span class="n">truncate_prompt_tokens</span>
        <span class="n">_validate_truncation_size</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">max_model_len</span><span class="p">,</span>
            <span class="n">truncate_prompt_tokens</span><span class="p">,</span>
            <span class="n">tokenization_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">prompt_str</span><span class="p">,</span> <span class="n">request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span><span class="o">.</span><span class="n">process_inputs</span><span class="p">(</span>
            <span class="n">request_id</span><span class="o">=</span><span class="n">request_id</span><span class="p">,</span>
            <span class="n">prompt</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">},</span>
            <span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sampling_params</span><span class="p">,</span>
            <span class="n">arrival_time</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">tokenization_kwargs</span><span class="o">=</span><span class="n">tokenization_kwargs</span><span class="p">,</span>
            <span class="n">trace_headers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">priority</span><span class="o">=</span><span class="n">priority</span><span class="p">,</span>
            <span class="n">data_parallel_rank</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># We do not support DP</span>
        <span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;process_inputs&quot;</span><span class="p">)</span>

        <span class="c1"># Wait until we&#39;re accepting requests (releases lock while waiting)</span>
        <span class="c1"># If accepting_requests is True, continue immediately (holding the lock)</span>
        <span class="c1"># If False, release lock, wait for notification, re-acquire and recheck</span>
        <span class="k">async</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_lock</span><span class="p">:</span>
            <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_lock</span><span class="o">.</span><span class="n">wait_for</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">accepting_requests</span><span class="p">)</span>

            <span class="c1"># Explicitly keeping the redundant logic to make it easier to pick up vLLM changes</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">num_samples</span> <span class="o">:=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampling_params</span><span class="o">.</span><span class="n">n</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output_processor</span><span class="o">.</span><span class="n">add_request</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">prompt_str</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                <span class="n">request</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preprocess_add_request</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
                <span class="n">request_fut</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Future</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requests</span><span class="p">[</span><span class="n">request_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">request_fut</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">add_request</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">parent_req</span> <span class="o">=</span> <span class="n">ParentRequest</span><span class="p">(</span><span class="n">request_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampling_params</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
                    <span class="c1"># Note: `get_child_info` mutates ParentRequest to track the</span>
                    <span class="c1"># generated child request</span>
                    <span class="n">child_request_id</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">parent_req</span><span class="o">.</span><span class="n">get_child_info</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                    <span class="n">child_request</span> <span class="o">=</span> <span class="n">request</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="n">num_samples</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">copy</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
                    <span class="n">child_request</span><span class="o">.</span><span class="n">request_id</span> <span class="o">=</span> <span class="n">child_request_id</span>
                    <span class="n">child_request</span><span class="o">.</span><span class="n">sampling_params</span> <span class="o">=</span> <span class="n">params</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">output_processor</span><span class="o">.</span><span class="n">add_request</span><span class="p">(</span>
                        <span class="n">child_request</span><span class="p">,</span> <span class="n">prompt_str</span><span class="p">,</span> <span class="n">parent_req</span><span class="p">,</span> <span class="n">idx</span>
                    <span class="p">)</span>
                    <span class="n">child_request</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preprocess_add_request</span><span class="p">(</span><span class="n">child_request</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">add_request</span><span class="p">(</span><span class="n">child_request</span><span class="p">)</span>
                <span class="n">request_fut</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Future</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requests</span><span class="p">[</span><span class="n">request_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">parent_req</span><span class="p">,</span> <span class="n">request_fut</span><span class="p">)</span>

        <span class="n">completions</span> <span class="o">=</span> <span class="k">await</span> <span class="n">request_fut</span>
        <span class="n">t</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="s2">&quot;generate&quot;</span><span class="p">)</span>

        <span class="c1"># Log some metrics</span>
        <span class="n">record_metric</span><span class="p">(</span>
            <span class="s2">&quot;generator/generate/count_sequences_completed&quot;</span><span class="p">,</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">completions</span><span class="p">),</span>
            <span class="n">Reduce</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">completion</span> <span class="ow">in</span> <span class="n">completions</span><span class="p">:</span>
            <span class="n">num_generated_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">completion</span><span class="o">.</span><span class="n">token_ids</span><span class="p">)</span>
            <span class="n">record_metric</span><span class="p">(</span>
                <span class="s2">&quot;generator/generate/sum_tokens_generated&quot;</span><span class="p">,</span>
                <span class="n">num_generated_tokens</span><span class="p">,</span>
                <span class="n">Reduce</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">record_metric</span><span class="p">(</span>
                <span class="s2">&quot;generator/generate/avg_tokens_generated&quot;</span><span class="p">,</span>
                <span class="n">num_generated_tokens</span><span class="p">,</span>
                <span class="n">Reduce</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">completions</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_preprocess_add_request</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">EngineCoreRequest</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Request</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;(forge/issues/332) Will require attention when we bump vllm versions</span>
<span class="sd">        https://github.com/vllm-project/vllm/blob/0e3bb543f064eb416bca4f6f3013efa3830b12f7/vllm/v1/engine/core.py#L419</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">request</span><span class="o">.</span><span class="n">mm_hashes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Support for mm_hash is not implemented yet.&quot;</span><span class="p">)</span>
        <span class="n">req</span> <span class="o">=</span> <span class="n">Request</span><span class="o">.</span><span class="n">from_engine_core_request</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">req</span><span class="o">.</span><span class="n">use_structured_output</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">structured_output_manager</span><span class="o">.</span><span class="n">grammar_init</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">req</span><span class="p">,</span> <span class="mi">0</span>

<div class="viewcode-block" id="Generator.run">
<a class="viewcode-back" href="../../../api_generator.html#forge.actors.generator.Generator.run">[docs]</a>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Schedule, execute, and make output.</span>
<span class="sd">        https://github.com/vllm-project/vllm/blob/0e3bb543f064eb416bca4f6f3013efa3830b12f7/vllm/v1/engine/core.py#L276</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: move postprocessing out of loop to not block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">running</span><span class="p">:</span>
            <span class="n">scheduler_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">schedule</span><span class="p">()</span>
            <span class="n">worker_outputs</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">execute_model</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">scheduler_output</span><span class="p">)</span>

            <span class="c1"># The results of `execute_model` are gathered on the driver rank (rank 0)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">worker_output</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">worker_outputs</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">update_from_output</span><span class="p">(</span><span class="n">scheduler_output</span><span class="p">,</span> <span class="n">worker_output</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="n">EngineCoreOutputs</span><span class="p">()</span>
            <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Release control before processing outputs</span>

            <span class="n">processed_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_processor</span><span class="o">.</span><span class="n">process_outputs</span><span class="p">(</span>
                <span class="n">outputs</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span>
                <span class="n">engine_core_timestamp</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">timestamp</span><span class="p">,</span>
                <span class="n">iteration_stats</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># TODO: add support for `iteration_stats`</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">request_output</span> <span class="ow">in</span> <span class="n">processed_outputs</span><span class="o">.</span><span class="n">request_outputs</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">request_output</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
                    <span class="n">completions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to_completions</span><span class="p">(</span><span class="n">request_output</span><span class="p">)</span>
                    <span class="n">_</span><span class="p">,</span> <span class="n">fut</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">requests</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">request_output</span><span class="o">.</span><span class="n">request_id</span><span class="p">)</span>
                    <span class="n">fut</span><span class="o">.</span><span class="n">set_result</span><span class="p">(</span><span class="n">completions</span><span class="p">)</span>

            <span class="c1"># Notify waiters if queue is drained</span>
            <span class="k">async</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_lock</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">requests</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">request_lock</span><span class="o">.</span><span class="n">notify_all</span><span class="p">()</span></div>


    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">update_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">version</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update weights on base model from a generator version to be found in a torchstore volume.</span>

<span class="sd">        Args:</span>
<span class="sd">            generator_version (int): Generator version from which to update. This will correspond to a key in a</span>
<span class="sd">                torchstore volume.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; trainer.train_step(...)</span>
<span class="sd">            &gt;&gt;&gt; version += 1</span>
<span class="sd">            &gt;&gt;&gt; await trainer.push_weights()</span>
<span class="sd">            &gt;&gt;&gt; generator.update_weights(version)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: enable shared memory prefetch for DCP-based weight sync</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_weights_to_shm</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_dcp_for_weight_sync</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Generator] Fetching weights for v</span><span class="si">{</span><span class="n">version</span><span class="si">}</span><span class="s2"> to shared memory&quot;</span><span class="p">)</span>
            <span class="n">fetch_fut</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">create_task</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fetch_weights</span><span class="p">(</span><span class="n">version</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fetch_fut</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Serialize updates (only one update at a time)</span>
        <span class="k">async</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_lock</span><span class="p">:</span>
            <span class="c1"># Grab the lock to stop accepting requests and wait on pending requests</span>
            <span class="k">async</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_lock</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accepting_requests</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">curr_requests</span> <span class="o">=</span> <span class="p">[</span><span class="n">fut</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">fut</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">requests</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
                <span class="k">if</span> <span class="n">curr_requests</span><span class="p">:</span>
                    <span class="c1"># Record pending requests metrics</span>
                    <span class="n">record_metric</span><span class="p">(</span>
                        <span class="s2">&quot;generator_perf/update_weights/avg_pending_requests&quot;</span><span class="p">,</span>
                        <span class="nb">len</span><span class="p">(</span><span class="n">curr_requests</span><span class="p">),</span>
                        <span class="n">Reduce</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">record_metric</span><span class="p">(</span>
                        <span class="s2">&quot;generator_perf/update_weights/max_pending_requests&quot;</span><span class="p">,</span>
                        <span class="nb">len</span><span class="p">(</span><span class="n">curr_requests</span><span class="p">),</span>
                        <span class="n">Reduce</span><span class="o">.</span><span class="n">MAX</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Waiting for </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">curr_requests</span><span class="p">)</span><span class="si">}</span><span class="s2"> pending requests&quot;</span><span class="p">)</span>

                <span class="c1"># Wait until all pending requests have been processed</span>
                <span class="c1"># TODO: If generating long sequences, this might be long and will block</span>
                <span class="c1"># generator weight updates</span>
                <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_lock</span><span class="o">.</span><span class="n">wait_for</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">requests</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>

            <span class="c1"># Record weight update metrics</span>
            <span class="n">record_metric</span><span class="p">(</span>
                <span class="s2">&quot;generator/update_weights/count_weight_updates&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Reduce</span><span class="o">.</span><span class="n">SUM</span>
            <span class="p">)</span>

            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting weight update on </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">fetch_fut</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">t</span> <span class="o">=</span> <span class="n">Tracer</span><span class="p">(</span><span class="s2">&quot;generator_perf/waiting_for_fetch_weights&quot;</span><span class="p">)</span>
                <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
                <span class="n">fetched_weights</span> <span class="o">=</span> <span class="k">await</span> <span class="n">fetch_fut</span>
                <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
                <span class="c1"># Call update_weights on every policy_worker</span>
                <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">update_weights</span><span class="o">.</span><span class="n">call</span><span class="p">(</span>
                    <span class="n">shared_memory_state_dict</span><span class="o">=</span><span class="n">fetched_weights</span>
                <span class="p">)</span>
                <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_drop_shared_memory</span><span class="p">(</span><span class="n">fetched_weights</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">update_weights</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generator_version</span> <span class="o">=</span> <span class="n">version</span>

            <span class="c1"># After updating the weights, we need to reset the KV cache</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">reset_prefix_cache</span><span class="p">()</span>

        <span class="c1"># Resume accepting requests and wake up any waiting generate() calls</span>
        <span class="k">async</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_lock</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accepting_requests</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">request_lock</span><span class="o">.</span><span class="n">notify_all</span><span class="p">()</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Weight update completed (now v</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">generator_version</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_reset_prefix_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">reset_prefix_cache</span><span class="p">()</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">get_version</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the current generator version.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator_version</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">stop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_to_completions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request_output</span><span class="p">:</span> <span class="n">RequestOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Completion</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert a vLLM RequestOutput to a list of Completion objects.&quot;&quot;&quot;</span>
        <span class="n">completions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">original_prompt</span> <span class="o">=</span> <span class="n">request_output</span><span class="o">.</span><span class="n">prompt</span>
        <span class="n">prompt_token_ids</span> <span class="o">=</span> <span class="n">request_output</span><span class="o">.</span><span class="n">prompt_token_ids</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">request_output</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
            <span class="n">completions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">Completion</span><span class="p">(</span>
                    <span class="c1"># TODO: the to_prompt encoding will be different from the original.</span>
                    <span class="c1"># This is okay for now, since I don&#39;t see any direct usage of prompt using completion object.</span>
                    <span class="n">prompt</span><span class="o">=</span><span class="n">to_prompt</span><span class="p">(</span><span class="n">original_prompt</span><span class="p">),</span>
                    <span class="n">stop_reason</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">finish_reason</span><span class="p">,</span>
                    <span class="n">text</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">text</span><span class="p">,</span>
                    <span class="n">prompt_ids</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">),</span>
                    <span class="n">token_ids</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">token_ids</span><span class="p">),</span>
                    <span class="n">logprobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_extract_logprobs</span><span class="p">(</span><span class="n">output</span><span class="p">),</span>
                    <span class="n">generator_version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">generator_version</span><span class="p">,</span>
                    <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_cached_tokens&quot;</span><span class="p">:</span> <span class="n">request_output</span><span class="o">.</span><span class="n">num_cached_tokens</span><span class="p">},</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">completions</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_extract_logprobs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">:</span> <span class="n">CompletionOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sample</span><span class="o">.</span><span class="n">logprobs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">top_k_dict</span><span class="p">[</span><span class="n">token</span><span class="p">]</span><span class="o">.</span><span class="n">logprob</span>
                    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">top_k_dict</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">sample</span><span class="o">.</span><span class="n">logprobs</span><span class="p">)</span>
                <span class="p">]</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

<div class="viewcode-block" id="Generator.shutdown">
<a class="viewcode-back" href="../../../api_generator.html#forge.actors.generator.Generator.shutdown">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">shutdown</span><span class="p">(</span>  <span class="c1"># pyright: ignore[reportIncompatibleMethodOverride]</span>
        <span class="bp">cls</span><span class="p">:</span> <span class="nb">type</span><span class="p">[</span><span class="s2">&quot;Generator&quot;</span><span class="p">],</span> <span class="n">actor</span><span class="p">:</span> <span class="s2">&quot;Generator&quot;</span>
    <span class="p">):</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">actor</span><span class="o">.</span><span class="n">_generator_proc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;Tried to shutdown a generator that was not initialized correctly&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">actor</span><span class="o">.</span><span class="n">_worker_procs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;Tried to shutdown a generator that was not initialized correctly&quot;</span>

        <span class="c1"># TODO - may want to expand stop to gracefully respond to</span>
        <span class="c1"># ongoing requests.</span>
        <span class="k">await</span> <span class="n">actor</span><span class="o">.</span><span class="n">stop</span><span class="o">.</span><span class="n">call</span><span class="p">()</span>
        <span class="k">await</span> <span class="n">stop_proc_mesh</span><span class="p">(</span><span class="n">actor</span><span class="o">.</span><span class="n">_worker_procs</span><span class="p">)</span>
        <span class="k">await</span> <span class="n">stop_proc_mesh</span><span class="p">(</span><span class="n">actor</span><span class="o">.</span><span class="n">_generator_proc</span><span class="p">)</span>
        <span class="k">await</span> <span class="n">stop_proc_mesh</span><span class="p">(</span><span class="n">actor</span><span class="o">.</span><span class="n">_fetcher_procs</span><span class="p">)</span></div>


    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_test_save_model_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save model parameters before weight update, used for tesing purposes only.&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;[Generator] save model parameters for testing.&quot;</span><span class="p">)</span>
        <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">_test_save_model_params</span><span class="o">.</span><span class="n">call</span><span class="p">()</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_test_validate_model_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">validate_fn</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Validate updated model params using validate_fn.&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;[Generator] start validating model parameters.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">_test_validate_model_params</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">validate_fn</span><span class="p">)</span></div>



<div class="viewcode-block" id="GeneratorWorker">
<a class="viewcode-back" href="../../../api_generator.html#forge.actors.generator.GeneratorWorker">[docs]</a>
<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">GeneratorWorker</span><span class="p">(</span><span class="n">ForgeActor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mirrors a vLLM GPUWorker</span>
<span class="sd">    https://github.com/vllm-project/vllm/blob/0e3bb543f064eb416bca4f6f3013efa3830b12f7/vllm/v1/worker/gpu_worker.py</span>

<span class="sd">    In general, this class should not be instantiated or called directly. Rather, the Generator controls</span>
<span class="sd">    the creation and invocation of all GeneratorWorker.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">vllm_config</span><span class="p">:</span> <span class="n">VllmConfig</span>
    <span class="c1"># TODO: Remove below param</span>
    <span class="n">_test_prev_params</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">current_rank</span><span class="p">()</span><span class="o">.</span><span class="n">rank</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RANK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
        <span class="n">parallel_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="o">.</span><span class="n">parallel_config</span>
        <span class="n">set_multiprocessing_worker_envs</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="n">ip</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">),</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">)</span>
        <span class="n">distributed_init_method</span> <span class="o">=</span> <span class="n">get_distributed_init_method</span><span class="p">(</span><span class="n">ip</span><span class="p">,</span> <span class="n">port</span><span class="p">)</span>
        <span class="n">all_kwargs</span> <span class="o">=</span> <span class="p">[{}]</span> <span class="o">*</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">world_size</span>
        <span class="n">local_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">%</span> <span class="n">torch</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
        <span class="n">is_driver_worker</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">tensor_parallel_size</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">all_kwargs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;vllm_config&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="p">,</span>
            <span class="s2">&quot;local_rank&quot;</span><span class="p">:</span> <span class="n">local_rank</span><span class="p">,</span>
            <span class="s2">&quot;rank&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
            <span class="s2">&quot;distributed_init_method&quot;</span><span class="p">:</span> <span class="n">distributed_init_method</span><span class="p">,</span>
            <span class="s2">&quot;is_driver_worker&quot;</span><span class="p">:</span> <span class="n">is_driver_worker</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker</span> <span class="o">=</span> <span class="n">WorkerWrapperBase</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">init_worker</span><span class="p">(</span><span class="n">all_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">init_device</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">load_model</span><span class="p">()</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">setup_kv_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">KVCacheConfig</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;https://github.com/vllm-project/vllm/blob/5c7fe25491825b95936c011a43337c7d4fb7e472/vllm/v1/engine/core.py#L199&quot;&quot;&quot;</span>
        <span class="n">kv_cache_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">get_kv_cache_spec</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">kv_cache_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">available_gpu_memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">determine_available_memory</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Attention free models don&#39;t need memory for kv cache</span>
            <span class="n">available_gpu_memory</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Get the kv cache tensor size</span>
        <span class="n">kv_cache_config</span> <span class="o">=</span> <span class="n">get_kv_cache_config</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="p">,</span> <span class="n">kv_cache_spec</span><span class="p">,</span> <span class="n">available_gpu_memory</span>
        <span class="p">)</span>
        <span class="c1"># TODO: unify configs across TorchStore</span>
        <span class="c1"># unify_kv_cache_configs(kv_cache_configs)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="o">.</span><span class="n">cache_config</span><span class="o">.</span><span class="n">num_gpu_blocks</span> <span class="o">=</span> <span class="n">kv_cache_config</span><span class="o">.</span><span class="n">num_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="o">.</span><span class="n">cache_config</span><span class="o">.</span><span class="n">num_cpu_blocks</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Initialize kv cache and warmup the execution:</span>
        <span class="c1"># from multiproc_executor.py:MultiprocExecutor.initialize_from_config</span>
        <span class="n">kv_cache_configs</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">world_size</span>
        <span class="n">kv_cache_configs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">]</span> <span class="o">=</span> <span class="n">kv_cache_config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">initialize_from_config</span><span class="p">(</span><span class="n">kv_cache_configs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">compile_or_warm_up_model</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">initialize_cache</span><span class="p">(</span><span class="n">kv_cache_config</span><span class="o">.</span><span class="n">num_blocks</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">kv_cache_config</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">execute_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schedule</span><span class="p">:</span> <span class="n">SchedulerOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelRunnerOutput</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">execute_model</span><span class="p">(</span><span class="n">schedule</span><span class="p">)</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">update_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">shared_memory_state_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">SharedTensorHandle</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">model_runner</span><span class="o">.</span><span class="n">model</span>
        <span class="k">if</span> <span class="n">shared_memory_state_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;[PolicyWorker] update weights from shared memory.&quot;</span><span class="p">)</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">Tracer</span><span class="p">(</span>
                <span class="s2">&quot;generator_worker_perf/update_weights_from_shared_memory&quot;</span><span class="p">,</span> <span class="n">timer</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span>
            <span class="p">)</span>
            <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
            <span class="n">loaded_weights</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param_handle</span> <span class="ow">in</span> <span class="n">shared_memory_state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># Use context manager for automatic cleanup</span>
                <span class="k">with</span> <span class="n">param_handle</span><span class="o">.</span><span class="n">to_shared_tensor</span><span class="p">()</span> <span class="k">as</span> <span class="n">shared_tensor</span><span class="p">:</span>
                    <span class="n">param</span> <span class="o">=</span> <span class="n">shared_tensor</span><span class="o">.</span><span class="n">tensor</span>
                    <span class="n">loaded</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">([(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="p">)])</span>
                    <span class="k">del</span> <span class="n">param</span>
                    <span class="n">loaded_weights</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loaded</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[PolicyWorker] updated </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">loaded_weights</span><span class="p">)</span><span class="si">}</span><span class="s2"> paremeters&quot;</span><span class="p">)</span>
            <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
            <span class="k">return</span>
        <span class="c1"># normal update_weights without shared memory prefetching</span>
        <span class="k">if</span> <span class="n">version</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;version must be provided if not using shared_memory_state_dict&quot;</span>
            <span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;[PolicyWorker] update weights from torchstore.&quot;</span><span class="p">)</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="n">get_param_prefix</span><span class="p">(</span><span class="n">version</span><span class="p">)</span>
        <span class="n">matching_keys</span> <span class="o">=</span> <span class="k">await</span> <span class="n">ts</span><span class="o">.</span><span class="n">keys</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span>
        <span class="n">dcp_whole_state_dict_key</span> <span class="o">=</span> <span class="n">get_dcp_whole_state_dict_key</span><span class="p">(</span><span class="n">version</span><span class="p">)</span>
        <span class="n">use_dcp_for_weight_sync</span> <span class="o">=</span> <span class="n">dcp_whole_state_dict_key</span> <span class="ow">in</span> <span class="n">matching_keys</span>
        <span class="n">loaded_weights</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">Tracer</span><span class="p">(</span><span class="s2">&quot;generator_worker_perf/update_weights_from_torchstore&quot;</span><span class="p">,</span> <span class="n">timer</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>
        <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">use_dcp_for_weight_sync</span><span class="p">:</span>
            <span class="n">dcp_handle</span> <span class="o">=</span> <span class="k">await</span> <span class="n">ts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">dcp_whole_state_dict_key</span><span class="p">)</span>
            <span class="n">hf_param_names</span> <span class="o">=</span> <span class="n">dcp_handle</span><span class="o">.</span><span class="n">param_names</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">hf_param_names</span><span class="p">:</span>
                <span class="n">param</span> <span class="o">=</span> <span class="n">load_tensor_from_dcp</span><span class="p">(</span><span class="n">dcp_handle</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                <span class="n">loaded</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">([(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="p">)])</span>
                <span class="k">del</span> <span class="n">param</span>
                <span class="n">loaded_weights</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loaded</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hf_param_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">extract_param_name</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">matching_keys</span><span class="p">]</span>
            <span class="c1"># We can&#39;t pass a generator since vllm load_weights is not async.</span>
            <span class="c1"># Instead, we just call load_weights with one parameter at a time.</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">hf_param_names</span><span class="p">:</span>
                <span class="n">param_key</span> <span class="o">=</span> <span class="n">get_param_key</span><span class="p">(</span><span class="n">version</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                <span class="n">param</span> <span class="o">=</span> <span class="k">await</span> <span class="n">ts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">param_key</span><span class="p">)</span>
                <span class="n">loaded</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">([(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="p">)])</span>
                <span class="k">del</span> <span class="n">param</span>
                <span class="n">loaded_weights</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loaded</span><span class="p">)</span>

        <span class="n">t</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_test_save_model_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save model parameters before weight update, used for tesing purposes only.&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;[GeneratorWorker] save model parameters for testing.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">model_runner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_test_prev_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;[GeneratorWorker] finished saving model parameters, len = </span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_test_prev_params</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_test_validate_model_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">validate_fn</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Validate updated model params using validate_fn.&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;[GeneratorWorker] start validating model parameters.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">validate_fn</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_test_prev_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">model_runner</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">logger</span>
        <span class="p">)</span></div>



<span class="k">class</span><span class="w"> </span><span class="nc">_WeightFetcher</span><span class="p">(</span><span class="n">ForgeActor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fetches weights from torchstore and loads them into shared memory.</span>
<span class="sd">    This has to be colocated with the GeneratorWorker.&quot;&quot;&quot;</span>

    <span class="nd">@endpoint</span>
    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">fetch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">version</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">param_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">SharedTensorHandle</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fetch weights from torchstore and load them into shared memory.&quot;&quot;&quot;</span>
        <span class="n">sd</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">param_names</span><span class="p">:</span>
            <span class="n">param_key</span> <span class="o">=</span> <span class="n">get_param_key</span><span class="p">(</span><span class="n">version</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
            <span class="n">param</span> <span class="o">=</span> <span class="k">await</span> <span class="n">ts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">param_key</span><span class="p">)</span>
            <span class="c1"># Use context manager to ensure cleanup after getting handle</span>
            <span class="k">with</span> <span class="n">SharedTensor</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">param</span><span class="p">)</span> <span class="k">as</span> <span class="n">shared_tensor</span><span class="p">:</span>
                <span class="n">handle</span> <span class="o">=</span> <span class="n">shared_tensor</span><span class="o">.</span><span class="n">get_handle</span><span class="p">()</span>
                <span class="n">sd</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">handle</span>
            <span class="k">del</span> <span class="n">param</span>  <span class="c1"># Explicitly free the tensor after copying to shared memory</span>
        <span class="k">return</span> <span class="n">sd</span>
</pre></div>

                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  


<style>
.site-footer {
    padding: 20px 40px;
    height: 60px !important;
}

@media screen and (min-width: 768px) {
    .site-footer {
        padding: 20px 40px;
    }
}

.site-footer .privacy-policy {
    border-top: none;
    margin-top: 0px;
}

.site-footer .privacy-policy .copyright {
    padding-top: 0;
}
</style>


<footer class="site-footer">

    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
           Copyright © 2025 Meta Platforms, Inc
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "forge.actors.generator",
       "headline": "forge.actors.generator",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/_modules/forge/actors/generator.html",
       "articleBody": "Source code for forge.actors.generator # Copyright (c) Meta Platforms, Inc. and affiliates. # All rights reserved. # # This source code is licensed under the BSD-style license found in the # LICENSE file in the root directory of this source tree. from __future__ import annotations import asyncio import logging import os import sys from collections.abc import Mapping from copy import copy from dataclasses import dataclass, field from typing import Optional import torch import torchstore as ts from monarch.actor import current_rank, endpoint, ProcMesh, this_host from vllm.config import VllmConfig from vllm.engine.arg_utils import EngineArgs from vllm.entrypoints.utils import _validate_truncation_size from vllm.executor.multiproc_worker_utils import set_multiprocessing_worker_envs from vllm.outputs import CompletionOutput, RequestOutput from vllm.sampling_params import RequestOutputKind, SamplingParams from vllm.transformers_utils.tokenizer_group import init_tokenizer_from_configs from vllm.usage.usage_lib import UsageContext from vllm.utils import get_distributed_init_method from vllm.v1.core.kv_cache_utils import get_kv_cache_config from vllm.v1.core.sched.output import SchedulerOutput from vllm.v1.core.sched.scheduler import Scheduler from vllm.v1.engine import EngineCoreOutputs, EngineCoreRequest from vllm.v1.engine.output_processor import OutputProcessor from vllm.v1.engine.parallel_sampling import ParentRequest from vllm.v1.engine.processor import Processor from vllm.v1.kv_cache_interface import KVCacheConfig from vllm.v1.outputs import ModelRunnerOutput from vllm.v1.request import Request from vllm.v1.structured_output import StructuredOutputManager from vllm.worker.worker_base import WorkerWrapperBase from forge.actors._torchstore_utils import ( extract_param_name, get_dcp_whole_state_dict_key, get_param_key, get_param_prefix, load_tensor_from_dcp, rdma_available, ) from forge.controller import ( ForgeActor, get_proc_mesh, host_mesh_from_proc, stop_proc_mesh, ) from forge.data_models.completion import Completion from forge.data_models.prompt import to_prompt from forge.observability.metrics import record_metric, Reduce from forge.observability.perf_tracker import Tracer from forge.types import ProcessConfig from forge.util._shared_tensor import SharedTensor, SharedTensorHandle logger = logging.getLogger(__name__) logger.setLevel(logging.INFO) [docs] @dataclass class Generator(ForgeActor): \"\"\"Instance of a vLLM-based generator. This class manually recreates a vLLM engine that mirrors the design of AsyncLLMEngine in v1. The main difference is that all communications are controlled here via Monarch\u0027s proc meshes. Args: engine_args (EngineArgs): The engine arguments to use for the vLLM engine. sampling_params (SamplingParams): The sampling parameters to use for the vLLM engine. use_dcp_for_weight_sync (bool): Whether to use DCP for NFS-based weight sync. Default depends on whether or not RDMA is enabled in torchstore. If it is, then DCP is disabled. Otherwise, DCP is enabled. Example: \u003e\u003e\u003e generator = await Generator.options(procs=1, num_replicas=1, with_gpus=True).as_service( ... engine_args=EngineArgs(...), ... sampling_params=SamplingParams(...), ... ) \u003e\u003e\u003e await generator.generate(\"Tell me a joke\") Completion(prompt=\"Tell me a joke\", text=\"A: Why did the chicken cross the road? B: To get to the other side.\", token_ids=[...], logprobs=[...]) \u003e\u003e\u003e await generator.shutdown() \"\"\" engine_args: EngineArgs | Mapping = field(default_factory=EngineArgs) sampling_params: SamplingParams | Mapping = field(default_factory=SamplingParams) use_dcp_for_weight_sync: bool | None = None prefetch_weights_to_shm: bool = True n_fetcher_procs: int = 8 def __post_init__(self): super().__init__() self._run_task: asyncio.Task | None = None self._generator_proc: ProcMesh | None = None self._worker_procs: ProcMesh | None = None self.worker: GeneratorWorker | None = None self.running = False self.generator_version: int = 0 if isinstance(self.engine_args, Mapping): self.engine_args = EngineArgs(**self.engine_args) self.engine_args._is_v1_supported_oracle = lambda *_: True self.vllm_config = self.engine_args.create_engine_config(UsageContext.LLM_CLASS) if isinstance(self.sampling_params, Mapping): self.sampling_params = SamplingParams.from_optional(**self.sampling_params) self.sampling_params.output_kind = RequestOutputKind.FINAL_ONLY if self.use_dcp_for_weight_sync is None: self.use_dcp_for_weight_sync = not rdma_available() logger.debug(f\"{self.use_dcp_for_weight_sync=}\") @endpoint async def get_vllm_config(self) -\u003e VllmConfig: return self.vllm_config @endpoint async def register_worker(self, worker: GeneratorWorker) -\u003e None: self.worker = worker logger.debug(\"Registered GeneratorWorker on Generator.\") @classmethod async def launch( # pyright: ignore[reportIncompatibleMethodOverride] cls: type[\"Generator\"], *args, **kwargs, ) -\u003e \"Generator\": \"\"\"Custom launch for the Generator service with its workers. We overwrite the default Service launch method in order to setup Actors (GeneratorWorker) within this \"coordinating\" Actor. We first create a proc_mesh for the workers, then a proc_mesh for the generator, and then we spawn the workers and the generator in setup. \"\"\" # Note: get_proc_mesh will set MASTER_ADDR, MASTER_PORT and CUDA_VISIBLE_DEVICES process_config: ProcessConfig = ProcessConfig( procs=cls.procs, hosts=cls.hosts, with_gpus=cls.with_gpus, mesh_name=cls.mesh_name, ) # First, spawn the worker processes which may or may not be # on remote hosts. worker_procs = await get_proc_mesh(process_config=process_config) # Then, grab a single host from the workers... host_mesh = await host_mesh_from_proc(worker_procs) singleton_slice = {k: slice(0, 1) for k in host_mesh.extent.keys()} host_mesh = host_mesh.slice(**singleton_slice) # We ask the provisioner for a single process on a single host generator_proc_config = copy(process_config) generator_proc_config.procs = 1 generator_proc_config.with_gpus = False generator_proc = await get_proc_mesh( process_config=generator_proc_config, host_mesh=host_mesh ) # TODO - expand support so name can stick within kwargs actor_name = kwargs.pop(\"name\", cls.__name__) generator = generator_proc.spawn( actor_name, cls, *args, **kwargs, ) vllm_config = ( await generator.get_vllm_config.call_one() ) # Config should be the same across all actors worker = worker_procs.spawn( \"vllm_worker\", GeneratorWorker, vllm_config=vllm_config ) await worker.setup.call() await generator.register_worker.call(worker) generator._generator_proc = generator_proc generator._worker_procs = worker_procs await generator.setup.call() return generator @endpoint async def setup(self): \"\"\"Mirrors the __init__ of vLLM\u0027s LLMEngine.\"\"\" self.request_id = 0 self.requests: dict[str, tuple[ParentRequest | None, asyncio.Future]] = {} # TODO: Investigate whether this can be combined with `generator.running` self.accepting_requests = True self.request_lock = asyncio.Condition() # Guard for accepting_requests self.update_lock = asyncio.Condition() # Guard for updating requests # Setup processors # TODO: move all processing to the Environment # TODO: add support for `log_stats` and `mm_registry` tokenizer = init_tokenizer_from_configs( model_config=self.vllm_config.model_config, scheduler_config=self.vllm_config.scheduler_config, lora_config=self.vllm_config.lora_config, ) self.processor = Processor( vllm_config=self.vllm_config, tokenizer=tokenizer, mm_registry=None ) self.output_processor = OutputProcessor(tokenizer, log_stats=None) # Configure KV caches kv_cache_configs = await self.worker.setup_kv_cache.call() _, kv_cache_config = next(kv_cache_configs.items()) self.vllm_config.cache_config.num_gpu_blocks = kv_cache_config.num_blocks self.vllm_config.cache_config.num_cpu_blocks = 0 # Setup scheduler # TODO: Add support for `log_stats` structured_output_manager = StructuredOutputManager(self.vllm_config) self.scheduler = Scheduler( vllm_config=self.vllm_config, kv_cache_config=kv_cache_config, structured_output_manager=structured_output_manager, include_finished_set=False, log_stats=None, ) self._start_processing() if self.prefetch_weights_to_shm: self._spawn_fetchers() def _spawn_fetchers(self): \"\"\"Spawn weight fetchers that prefetch weights from torchstore to shared memory.\"\"\" # TODO: this assumes the generator is on the same host as the worker # and only works for single host generators. Figure out how to support # generators with workers spanned across multiple hosts. fetcher_procs = this_host().spawn_procs( per_host={\"procs\": self.n_fetcher_procs} ) self._fetcher_procs = fetcher_procs self.weight_fetchers = fetcher_procs.spawn(\"weight_fetcher\", _WeightFetcher) def _start_processing(self): if self._run_task is None or self._run_task.done(): self._run_task = asyncio.create_task(self.run()) async def _drop_shared_memory(self, state_dict: dict[str, SharedTensorHandle]): for handle in state_dict.values(): handle.drop() async def _fetch_weights( self, version: int, ) -\u003e dict[str, SharedTensorHandle]: \"\"\"Fetch weights from torchstore and return a dict of {name: SharedTensorHandle}.\"\"\" t = Tracer(\"generator_perf/_fetch_weights\") t.start() prefix = get_param_prefix(version) matching_keys = await ts.keys(prefix) hf_param_names = [extract_param_name(key) for key in matching_keys] n_fetchers = self.weight_fetchers.size() def split_keys(keys): return [keys[i::n_fetchers] for i in range(n_fetchers)] futures = [] for i, names in enumerate(split_keys(hf_param_names)): fut = self.weight_fetchers.slice(procs=i).fetch.call_one( version=version, param_names=names ) futures.append(fut) sub_state_dicts = [await fut for fut in futures] state_dict = {} for sd in sub_state_dicts: state_dict.update(sd) t.stop() return state_dict @endpoint async def generate(self, prompt: str, *, priority: int = 0) -\u003e list[Completion]: \"\"\"Generate a response for the given prompt Args: prompt (str): The prompt to generate a response for. priority (int, optional): The priority of the request. Defaults to 0. Returns: list[Completion]: n completions from vLLM based on your prompt. \"\"\" t = Tracer(\"generator_perf/generate\", timer=\"gpu\") t.start() record_metric(\"generator/generate/count_requests\", 1, Reduce.SUM) self.request_id += 1 % sys.maxsize request_id = str(self.request_id) tokenization_kwargs = {} # TODO: add truncation support https://github.com/vllm-project/vllm/issues/4507 truncate_prompt_tokens = self.sampling_params.truncate_prompt_tokens _validate_truncation_size( self.vllm_config.model_config.max_model_len, truncate_prompt_tokens, tokenization_kwargs, ) prompt_str, request = self.processor.process_inputs( request_id=request_id, prompt={\"prompt\": prompt}, params=self.sampling_params, arrival_time=None, tokenization_kwargs=tokenization_kwargs, trace_headers=None, priority=priority, data_parallel_rank=None, # We do not support DP ) t.step(\"process_inputs\") # Wait until we\u0027re accepting requests (releases lock while waiting) # If accepting_requests is True, continue immediately (holding the lock) # If False, release lock, wait for notification, re-acquire and recheck async with self.request_lock: await self.request_lock.wait_for(lambda: self.accepting_requests) # Explicitly keeping the redundant logic to make it easier to pick up vLLM changes if (num_samples := self.sampling_params.n) == 1: self.output_processor.add_request(request, prompt_str, None, 0) request, _ = self._preprocess_add_request(request) request_fut = asyncio.Future() self.requests[request_id] = (None, request_fut) self.scheduler.add_request(request) else: parent_req = ParentRequest(request_id, self.sampling_params) for idx in range(num_samples): # Note: `get_child_info` mutates ParentRequest to track the # generated child request child_request_id, params = parent_req.get_child_info(idx) child_request = request if idx == num_samples - 1 else copy(request) child_request.request_id = child_request_id child_request.sampling_params = params self.output_processor.add_request( child_request, prompt_str, parent_req, idx ) child_request, _ = self._preprocess_add_request(child_request) self.scheduler.add_request(child_request) request_fut = asyncio.Future() self.requests[request_id] = (parent_req, request_fut) completions = await request_fut t.step(\"generate\") # Log some metrics record_metric( \"generator/generate/count_sequences_completed\", len(completions), Reduce.SUM, ) for completion in completions: num_generated_tokens = len(completion.token_ids) record_metric( \"generator/generate/sum_tokens_generated\", num_generated_tokens, Reduce.SUM, ) record_metric( \"generator/generate/avg_tokens_generated\", num_generated_tokens, Reduce.MEAN, ) t.stop() return completions def _preprocess_add_request( self, request: EngineCoreRequest ) -\u003e tuple[Request, int]: \"\"\"(forge/issues/332) Will require attention when we bump vllm versions https://github.com/vllm-project/vllm/blob/0e3bb543f064eb416bca4f6f3013efa3830b12f7/vllm/v1/engine/core.py#L419 \"\"\" if request.mm_hashes is not None: raise NotImplementedError(\"Support for mm_hash is not implemented yet.\") req = Request.from_engine_core_request(request) if req.use_structured_output: self.scheduler.structured_output_manager.grammar_init(request) return req, 0 [docs] async def run(self) -\u003e None: \"\"\"Schedule, execute, and make output. https://github.com/vllm-project/vllm/blob/0e3bb543f064eb416bca4f6f3013efa3830b12f7/vllm/v1/engine/core.py#L276 \"\"\" # TODO: move postprocessing out of loop to not block self.running = True while self.running: scheduler_output = self.scheduler.schedule() worker_outputs = await self.worker.execute_model.call(scheduler_output) # The results of `execute_model` are gathered on the driver rank (rank 0) _, worker_output = next(worker_outputs.items()) outputs = self.scheduler.update_from_output(scheduler_output, worker_output) outputs = outputs.get(0) or EngineCoreOutputs() await asyncio.sleep(0) # Release control before processing outputs processed_outputs = self.output_processor.process_outputs( outputs.outputs, engine_core_timestamp=outputs.timestamp, iteration_stats=None, # TODO: add support for `iteration_stats` ) for request_output in processed_outputs.request_outputs: if request_output.finished: completions = self._to_completions(request_output) _, fut = self.requests.pop(request_output.request_id) fut.set_result(completions) # Notify waiters if queue is drained async with self.request_lock: if len(self.requests) == 0: self.request_lock.notify_all() @endpoint async def update_weights(self, version: int) -\u003e None: \"\"\"Update weights on base model from a generator version to be found in a torchstore volume. Args: generator_version (int): Generator version from which to update. This will correspond to a key in a torchstore volume. Example: \u003e\u003e\u003e trainer.train_step(...) \u003e\u003e\u003e version += 1 \u003e\u003e\u003e await trainer.push_weights() \u003e\u003e\u003e generator.update_weights(version) \"\"\" # TODO: enable shared memory prefetch for DCP-based weight sync if self.prefetch_weights_to_shm and not self.use_dcp_for_weight_sync: logger.info(f\"[Generator] Fetching weights for v{version} to shared memory\") fetch_fut = asyncio.create_task(self._fetch_weights(version)) else: fetch_fut = None # Serialize updates (only one update at a time) async with self.update_lock: # Grab the lock to stop accepting requests and wait on pending requests async with self.request_lock: self.accepting_requests = False curr_requests = [fut for _, fut in self.requests.values()] if curr_requests: # Record pending requests metrics record_metric( \"generator_perf/update_weights/avg_pending_requests\", len(curr_requests), Reduce.MEAN, ) record_metric( \"generator_perf/update_weights/max_pending_requests\", len(curr_requests), Reduce.MAX, ) logger.debug(f\"Waiting for {len(curr_requests)} pending requests\") # Wait until all pending requests have been processed # TODO: If generating long sequences, this might be long and will block # generator weight updates await self.request_lock.wait_for(lambda: len(self.requests) == 0) # Record weight update metrics record_metric( \"generator/update_weights/count_weight_updates\", 1, Reduce.SUM ) logger.debug(f\"Starting weight update on {self.__class__.__name__}\") if fetch_fut is not None: t = Tracer(\"generator_perf/waiting_for_fetch_weights\") t.start() fetched_weights = await fetch_fut t.stop() # Call update_weights on every policy_worker await self.worker.update_weights.call( shared_memory_state_dict=fetched_weights ) await self._drop_shared_memory(fetched_weights) else: await self.worker.update_weights.call(version=version) self.generator_version = version # After updating the weights, we need to reset the KV cache self.scheduler.reset_prefix_cache() # Resume accepting requests and wake up any waiting generate() calls async with self.request_lock: self.accepting_requests = True self.request_lock.notify_all() logger.info(f\"Weight update completed (now v{self.generator_version})\") @endpoint async def _reset_prefix_cache(self): self.scheduler.reset_prefix_cache() @endpoint async def get_version(self) -\u003e int: \"\"\"Get the current generator version.\"\"\" return self.generator_version @endpoint async def stop(self): self.running = False def _to_completions(self, request_output: RequestOutput) -\u003e list[Completion]: \"\"\"Convert a vLLM RequestOutput to a list of Completion objects.\"\"\" completions = [] original_prompt = request_output.prompt prompt_token_ids = request_output.prompt_token_ids for output in request_output.outputs: completions.append( Completion( # TODO: the to_prompt encoding will be different from the original. # This is okay for now, since I don\u0027t see any direct usage of prompt using completion object. prompt=to_prompt(original_prompt), stop_reason=output.finish_reason, text=output.text, prompt_ids=torch.tensor(prompt_token_ids), token_ids=torch.tensor(output.token_ids), logprobs=self._extract_logprobs(output), generator_version=self.generator_version, metadata={\"num_cached_tokens\": request_output.num_cached_tokens}, ) ) return completions def _extract_logprobs(self, sample: CompletionOutput) -\u003e torch.Tensor | None: if sample.logprobs is not None: return torch.tensor( [ top_k_dict[token].logprob for token, top_k_dict in zip(sample.token_ids, sample.logprobs) ] ) return None [docs] @classmethod async def shutdown( # pyright: ignore[reportIncompatibleMethodOverride] cls: type[\"Generator\"], actor: \"Generator\" ): assert ( actor._generator_proc is not None ), \"Tried to shutdown a generator that was not initialized correctly\" assert ( actor._worker_procs is not None ), \"Tried to shutdown a generator that was not initialized correctly\" # TODO - may want to expand stop to gracefully respond to # ongoing requests. await actor.stop.call() await stop_proc_mesh(actor._worker_procs) await stop_proc_mesh(actor._generator_proc) await stop_proc_mesh(actor._fetcher_procs) @endpoint async def _test_save_model_params(self): \"\"\"Save model parameters before weight update, used for tesing purposes only.\"\"\" logger.info(\"[Generator] save model parameters for testing.\") await self.worker._test_save_model_params.call() @endpoint async def _test_validate_model_params(self, validate_fn): \"\"\"Validate updated model params using validate_fn.\"\"\" logger.info(\"[Generator] start validating model parameters.\") return await self.worker._test_validate_model_params.call(validate_fn) [docs] @dataclass class GeneratorWorker(ForgeActor): \"\"\"Mirrors a vLLM GPUWorker https://github.com/vllm-project/vllm/blob/0e3bb543f064eb416bca4f6f3013efa3830b12f7/vllm/v1/worker/gpu_worker.py In general, this class should not be instantiated or called directly. Rather, the Generator controls the creation and invocation of all GeneratorWorker. \"\"\" vllm_config: VllmConfig # TODO: Remove below param _test_prev_params = {} @endpoint async def setup(self): self.rank = current_rank().rank os.environ[\"RANK\"] = str(self.rank) parallel_config = self.vllm_config.parallel_config set_multiprocessing_worker_envs(parallel_config) ip, port = os.getenv(\"MASTER_ADDR\"), os.getenv(\"MASTER_PORT\") distributed_init_method = get_distributed_init_method(ip, port) all_kwargs = [{}] * parallel_config.world_size local_rank = self.rank % torch.accelerator.device_count() is_driver_worker = self.rank % parallel_config.tensor_parallel_size == 0 all_kwargs[self.rank] = { \"vllm_config\": self.vllm_config, \"local_rank\": local_rank, \"rank\": self.rank, \"distributed_init_method\": distributed_init_method, \"is_driver_worker\": is_driver_worker, } self.worker = WorkerWrapperBase(self.vllm_config, self.rank) self.worker.init_worker(all_kwargs) self.worker.init_device() self.worker.load_model() @endpoint async def setup_kv_cache(self) -\u003e KVCacheConfig: \"\"\"https://github.com/vllm-project/vllm/blob/5c7fe25491825b95936c011a43337c7d4fb7e472/vllm/v1/engine/core.py#L199\"\"\" kv_cache_spec = self.worker.get_kv_cache_spec() if kv_cache_spec is not None: available_gpu_memory = self.worker.determine_available_memory() else: # Attention free models don\u0027t need memory for kv cache available_gpu_memory = 0 # Get the kv cache tensor size kv_cache_config = get_kv_cache_config( self.vllm_config, kv_cache_spec, available_gpu_memory ) # TODO: unify configs across TorchStore # unify_kv_cache_configs(kv_cache_configs) self.vllm_config.cache_config.num_gpu_blocks = kv_cache_config.num_blocks self.vllm_config.cache_config.num_cpu_blocks = 0 # Initialize kv cache and warmup the execution: # from multiproc_executor.py:MultiprocExecutor.initialize_from_config kv_cache_configs = [None] * self.vllm_config.parallel_config.world_size kv_cache_configs[self.rank] = kv_cache_config self.worker.initialize_from_config(kv_cache_configs) self.worker.compile_or_warm_up_model() self.worker.initialize_cache(kv_cache_config.num_blocks, 0) return kv_cache_config @endpoint async def execute_model(self, schedule: SchedulerOutput) -\u003e ModelRunnerOutput: return self.worker.execute_model(schedule) @endpoint async def update_weights( self, version: Optional[int] = None, *, shared_memory_state_dict: Optional[dict[str, SharedTensorHandle]] = None, ) -\u003e None: model = self.worker.model_runner.model if shared_memory_state_dict is not None: logger.info(\"[PolicyWorker] update weights from shared memory.\") t = Tracer( \"generator_worker_perf/update_weights_from_shared_memory\", timer=\"gpu\" ) t.start() loaded_weights = set() for name, param_handle in shared_memory_state_dict.items(): # Use context manager for automatic cleanup with param_handle.to_shared_tensor() as shared_tensor: param = shared_tensor.tensor loaded = model.load_weights([(name, param)]) del param loaded_weights.update(loaded) logger.info(f\"[PolicyWorker] updated {len(loaded_weights)} paremeters\") t.stop() return # normal update_weights without shared memory prefetching if version is None: raise ValueError( \"version must be provided if not using shared_memory_state_dict\" ) logger.info(\"[PolicyWorker] update weights from torchstore.\") prefix = get_param_prefix(version) matching_keys = await ts.keys(prefix) dcp_whole_state_dict_key = get_dcp_whole_state_dict_key(version) use_dcp_for_weight_sync = dcp_whole_state_dict_key in matching_keys loaded_weights = set() t = Tracer(\"generator_worker_perf/update_weights_from_torchstore\", timer=\"gpu\") t.start() if use_dcp_for_weight_sync: dcp_handle = await ts.get(dcp_whole_state_dict_key) hf_param_names = dcp_handle.param_names for name in hf_param_names: param = load_tensor_from_dcp(dcp_handle, name) loaded = model.load_weights([(name, param)]) del param loaded_weights.update(loaded) else: hf_param_names = [extract_param_name(key) for key in matching_keys] # We can\u0027t pass a generator since vllm load_weights is not async. # Instead, we just call load_weights with one parameter at a time. for name in hf_param_names: param_key = get_param_key(version, name) param = await ts.get(param_key) loaded = model.load_weights([(name, param)]) del param loaded_weights.update(loaded) t.stop() @endpoint async def _test_save_model_params(self): \"\"\"Save model parameters before weight update, used for tesing purposes only.\"\"\" logger.info(\"[GeneratorWorker] save model parameters for testing.\") for name, param in self.worker.model_runner.model.named_parameters(): self._test_prev_params[name] = param.detach().cpu() logger.info( \"[GeneratorWorker] finished saving model parameters, len = %d\", len(self._test_prev_params), ) @endpoint async def _test_validate_model_params(self, validate_fn): \"\"\"Validate updated model params using validate_fn.\"\"\" logger.info(\"[GeneratorWorker] start validating model parameters.\") return validate_fn( self._test_prev_params, self.worker.model_runner.model, logger ) class _WeightFetcher(ForgeActor): \"\"\"Fetches weights from torchstore and loads them into shared memory. This has to be colocated with the GeneratorWorker.\"\"\" @endpoint async def fetch( self, *, version: int, param_names: list[str], ) -\u003e dict[str, SharedTensorHandle]: \"\"\"Fetch weights from torchstore and load them into shared memory.\"\"\" sd = {} for name in param_names: param_key = get_param_key(version, name) param = await ts.get(param_key) # Use context manager to ensure cleanup after getting handle with SharedTensor(tensor=param) as shared_tensor: handle = shared_tensor.get_handle() sd[name] = handle del param # Explicitly free the tensor after copying to shared memory return sd",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../../../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/_modules/forge/actors/generator.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>